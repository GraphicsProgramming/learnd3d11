{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LearnD3D11","text":""},{"location":"#goal","title":"Goal","text":"<p>This is LearnD3D11, a guide aimed at anyone trying to learn Direct3D11, commonly referred to as DirectX 11. This guide explains the basic usage of Direct3D11 and general graphics programming topics and techniques, without requiring any previous experience within this field. The guide's code itself is written in C++, although if one has experience with C++ and can understand how we use it, it is probably easily transferable to any other language.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>We are just a bunch of graphics programming enthusiasts doing all this on our own time. This project is also still a work in progress; a lot of content may be missing or faulty. If you find typos, bugs or have questions, don't hesitate to open an issue or even send a PR. Also feel free to join us on our Discord server.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>To get started reading LearnD3D11, you can jump right into the first chapter!</p>"},{"location":"about/","title":"About Learn D3D11","text":""},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>We are just a bunch of graphics programming enthusiasts doing all this on our own time. If you find typos, bugs or have questions, don't hesitate to open an issue or even send a PR, otherwise feel free to join us on our Discord server.</p> <p>Join Graphics Programming</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2022 Graphics Programming\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-0-overview/","title":"Overview","text":"<p>As you may or may not know, there's a few choices in terms of what you can use for rendering, such as OpenGL, Vulkan, Metal, and what is commonly known as \"DirectX\". However, in word of mouth DirectX usually refers to Direct3D, which is one of the many API's that DirectX has. In fact DirectX provides tooling and libraries for more aspects of game development, including:</p> <ul> <li>Audio</li> <li>Fonts</li> <li>Input</li> <li>Graphics</li> </ul> <p>The goal of LearnD3D11 is, as one might guess, the targeted explanation and showcase of Direct3D11 within C++, or as one generally refers to it: \"DirectX 11\". This guide does NOT cover DirectX 12, the newer version of the API, as it is very low level and will usually require a good understanding of the GPU and how it works. We will cover concepts and some techniques used in graphics programming, and we do not expect you to have any prerequisites in this field. It is, however, not a guide covering C++ or programming in general; we expect at least the ability and understanding to write object-oriented programs in C++.</p> <p>During each step we'll provide a project for you to follow along as we explain everything for you to start rendering geometry using your GPU.</p> <p>Also note that DirectX is made by Microsoft and is generally only available on Windows. However, <code>DXVK</code> was developed to run D3D9 through D3D11 on Linux or Wine on top of Vulkan and would be the only way of developing and using D3D11 on those platforms. </p> <p>This initial section will cover creating the actual window, initializing Direct3D11 and getting our very first visuals (which is commonly known as the Hello Triangle)</p>"},{"location":"1-introduction/1-1-getting-started/1-1-0-overview/#project-structure","title":"Project structure","text":"<p>A few words about how each project will look like.</p> <p>Inside the project folder:</p> <pre><code>Assets/\nAssets/Models/\nAssets/Shaders/\nAssets/Textures/\nbin/Debug/\nbin/Release/\nobj/Debug/\nobj/Release/\nMain.cpp\nx-x-x-project.vcxproj.filters\nx-x-x-project.vcxproj.user\nx-x-x-project.vcxproj\n</code></pre> <ul> <li>The <code>vcxproj</code> files are part of Visual Studio's project system.</li> <li><code>Main.cpp</code> is the entry point of each application. (in the first few chapters we will have all the code in there, but later refactor them out into their own units)</li> <li><code>obj/</code> contains all intermediate junk the compiler produced, to keep the folder structure clean</li> <li><code>bin/</code> will contain the compiled program of the chapter along with all necessary <code>Assets</code></li> <li><code>Assets/</code> will contain all the used assets, such as models, shaders and textures and other things. It will be empty for the first few chapters, and we will copy it and its contents to the bin/Debug or bin/Release directory, depending on which configuration you chose</li> </ul>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/","title":"Hello Window","text":"<p>Note</p> <p>If you want to try to follow this series without downloading the full source, you can try this empty project, it has been setup already with all the include directories and libraries you will need throughout the whole series!</p> <p>Link to the full source code</p> <p>Let's start with the whole example code for creating a simple window first. We use the GLFW library to create our window and as you can probably see, it's really not that much code. The following code opens a new blank window titled <code>\"LearnD3D11 - Hello Window\"</code> and will leave it open until you hit close.</p> <pre><code>#include &lt;GLFW/glfw3.h&gt;\n#include &lt;cstdint&gt;\n#include &lt;iostream&gt;\n\nint main(int argc, char* argv[])\n{\n    if (!glfwInit())\n    {\n        std::cout &lt;&lt; \"GLFW: Unable to initialize\\n\";\n        return -1;\n    }\n\n    GLFWmonitor* primaryMonitor = glfwGetPrimaryMonitor();\n    const GLFWvidmode* videoMode = glfwGetVideoMode(primaryMonitor);\n    const int32_t width = static_cast&lt;int32_t&gt;(videoMode-&gt;width * 0.9f);\n    const int32_t height = static_cast&lt;int32_t&gt;(videoMode-&gt;height * 0.9f);\n\n    glfwWindowHint(GLFW_SCALE_TO_MONITOR, GLFW_FALSE);\n    glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\n    GLFWwindow* window = glfwCreateWindow(\n        width,\n        height,\n        \"LearnD3D11 - Hello Window\",\n        nullptr,\n        nullptr);\n    if (window == nullptr)\n    {\n        std::cout &lt;&lt; \"GLFW: Unable to create window\\n\";\n        glfwTerminate();\n        return -1;\n    }\n\n    const int32_t windowLeft = videoMode-&gt;width / 2 - width / 2;\n    const int32_t windowTop = videoMode-&gt;height / 2 - height / 2;\n    glfwSetWindowPos(window, windowLeft, windowTop);\n\n    while (!glfwWindowShouldClose(window))\n    {\n        glfwPollEvents();\n        // future update code\n        // future render code\n    }\n\n    glfwDestroyWindow(window);\n    glfwTerminate();\n    return 0;\n}\n</code></pre> <p>Now let's go over everything in that example in more detail.</p> <pre><code>#include &lt;GLFW/glfw3.h&gt;\n</code></pre> <p>C++ needs to know where all the definitions and declarations are coming from. We therefore have to include the GLFW header so that everything we need is present in our source file.</p> <pre><code>if (!glfwInit())\n{\n    std::cout &lt;&lt; \"GLFW: Unable to initialize\\n\";\n    return -1;\n}\n</code></pre> <p>Pretty obvious, right? glfwInit tries to initialize <code>GLFW</code>. If it fails to do so, let the user know and end the program, since there is no point in going further.</p> <pre><code>GLFWmonitor* primaryMonitor = glfwGetPrimaryMonitor();\nconst GLFWvidmode* videoMode = glfwGetVideoMode(primaryMonitor);\nconst int32_t width = static_cast&lt;int32_t&gt;(videoMode-&gt;width * 0.9f);\nconst int32_t height = static_cast&lt;int32_t&gt;(videoMode-&gt;height * 0.9f);\n</code></pre> <p>This piece of code grabs the main monitor via glfwGetPrimaryMonitor and its current resolution with glfwGetVideoMode, so that we can derive a window width and height from it - and it will look similar no matter what resolution you use. Size-wise it will cover 90% of your main monitor.</p> <pre><code>glfwWindowHint(GLFW_SCALE_TO_MONITOR, GLFW_FALSE);\n</code></pre> <p>This will tell <code>GLFW</code> to not scale the window in any way, should you have set up a specific scaling other than 100% on your desktop. That will keep the window size at what we set it, and lets us forget about fractional window and pixel scaling.</p> <pre><code>glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\n</code></pre> <p><code>GLFW</code> was initially meant to support development of OpenGL based applications, hence the gl in its name, but over the years it also started to support other APIs and not just OpenGL. Now since <code>GLFW</code> by default creates a context for OpenGL, and as we want to use DirectX we need to tell <code>GLFW</code> to not do so via glfwWindowHint.</p> <p>There are many other options one can define through glfwWindowHint which can be found here. Many of these options might be useful in your application, depending on what you want and how you want to design your window.</p> <pre><code>GLFWwindow* window = glfwCreateWindow(\n    width,\n    height,\n    \"LearnD3D11 - Hello Window\",\n    nullptr,\n    nullptr);\nif (window == nullptr)\n{\n    std::cout &lt;&lt; \"GLFW: Unable to create window\\n\";\n    glfwTerminate();\n    return -1;\n}\n</code></pre> <p>This piece actually creates the window, if everything goes well. We pass in desired window dimensions and a title, and call glfwCreateWindow. Make sure to check the return value, window creation can fail.</p> <pre><code>const int32_t windowLeft = videoMode-&gt;width / 2 - width / 2;\nconst int32_t windowTop = videoMode-&gt;height / 2 - height / 2;\nglfwSetWindowPos(window, windowLeft, windowTop);\n</code></pre> <p>GLFW does not center windows automatically, like you can with other libraries like <code>SDL</code>, there for we will center the window manually. All we need is just a bit of math an glfwSetWindowPos. It sets the window position in screen coordinates, specified by the top left corner of the window.</p> <pre><code>while (!glfwWindowShouldClose(window))\n{\n    glfwPollEvents();\n    // future update code\n    // future render code\n}\n</code></pre> <p>That is more or less the heart of your application, the mainloop. You could also call it game loop, since in here everything happens. From reading keyboard and mouse input, reacting to it, to telling the graphics card to put a frog on the screen. It will keep doing it, until it gets signaled to not do that anymore because you closed the window for example (glfwWindowShouldClose), or hit Escape and mapped Escape to close the window. glfwPollEvents will make sure that <code>GLFW</code> knows about all required events coming from the operating system.</p> <pre><code>glfwDestroyWindow(window);\nglfwTerminate();\nreturn 0;\n</code></pre> <p>Now we clean up the resources we have created, such as the window itself and the <code>GLFW</code> system. Then simply return to the OS, without any error.</p> <p>glfwDestroyWindow will obviously destroy the window and glfwTerminate cleans up <code>GLFW</code>.</p> <p>When you start the program, you should see something like this.</p> <p></p> <p>Why GLFW?</p> <p>We have decided to include GLFW to manage our window (and later input) because it is a very simple library, ready to use and above all, because of consistency, plenty of other resources use GLFW, so it should be easier to transfer the knowledge you will gain here elsewhere.</p> <p>There are other libraries, which handle windowing and input (amongst other things) out there as well, here are a few to choose from:</p> <ul> <li>SDL2</li> <li>SFML</li> </ul> <p>You could also write all that using the native functions and constructs the OS provides, on windows it would be WinAPI, and although it is old and not necessary to do all the heavylifting yourself, a lot of Microsoft's own guides rely on the WinAPI so we have an example on that. We will not be covering any other Win32 topics, unless it is explicitly required</p> <p>You are probably going to continue writing your own engine after this tutorial and might decide to support more platforms, other than just windows, then this is already covered by <code>GLFW</code>.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#first-abstraction","title":"First abstraction","text":"<p>The further we go into this tutorial series the more stuff we will add to the program. But we don't want to cram everything into <code>Main.cpp</code>, your main entry point of the program. A good practise is to split up things into smaller units, to not lose overview.</p> <p>Right now we don't have much to show for, just a window, made by a few lines of code, but we are going to abstract that code into a new class called <code>Application</code> which will also be our main container so to speak, in which all the magic will happen.</p> <p>I will show the whole code first, and then explain again what means what.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#applicationhpp","title":"Application.hpp","text":"<pre><code>#pragma once\n\n#include &lt;string&gt;\n#include &lt;iostream&gt;\n\nstruct GLFWwindow;\n\nclass Application\n{\npublic:\n    Application(const std::string&amp; title);\n    virtual ~Application();\n    void Run();\n\nprotected:\n    virtual void Cleanup();\n    virtual bool Initialize();\n    virtual bool Load() = 0;\n    virtual void Render() = 0;\n    virtual void Update() = 0;\n\nprivate:\n    GLFWwindow* _window = nullptr;\n    int32_t _width = 0;\n    int32_t _height = 0;\n    std::string_view _title;\n};\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#applicationcpp","title":"Application.cpp","text":"<pre><code>#include \"Application.hpp\"\n#include &lt;GLFW/glfw3.h&gt;\n\nApplication::Application(const std::string&amp; title)\n{\n    _title = title;\n}\n\nApplication::~Application()\n{\n    Cleanup();\n}\n\nvoid Application::Run()\n{\n    if (!Initialize())\n    {\n        return;\n    }\n\n    while (!glfwWindowShouldClose(_window))\n    {\n        glfwPollEvents();\n        Update();\n        Render();\n    }\n}\n\nvoid Application::Cleanup()\n{\n    if (_window != nullptr)\n    {\n        glfwDestroyWindow(_window);\n        _window = nullptr;\n    }\n    glfwTerminate();\n}\n\nbool Application::Initialize()\n{\n    if (!glfwInit())\n    {\n        std::cout &lt;&lt; \"GLFW: Unable to initialize\\n\";\n        return false;\n    }\n\n    GLFWmonitor* primaryMonitor = glfwGetPrimaryMonitor();\n    const GLFWvidmode* videoMode = glfwGetVideoMode(primaryMonitor);\n    _width = static_cast&lt;int32_t&gt;(videoMode-&gt;width * 0.9f);\n    _height = static_cast&lt;int32_t&gt;(videoMode-&gt;height * 0.9f);\n\n    glfwWindowHint(GLFW_SCALE_TO_MONITOR, GLFW_FALSE);\n    glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\n    _window = glfwCreateWindow(_width, _height, _title.data(), nullptr, nullptr);\n    if (_window == nullptr)\n    {\n        std::cout &lt;&lt; \"GLFW: Unable to create window\\n\";\n        return false;\n    }\n\n    const int32_t windowLeft = videoMode-&gt;width / 2 - _width / 2;\n    const int32_t windowTop = videoMode-&gt;height / 2 - _height / 2;\n    glfwSetWindowPos(_window, windowLeft, windowTop);\n\n    return true;\n}\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#hellowindowapplicationhpp","title":"HelloWindowApplication.hpp","text":"<pre><code>#include \"Application.hpp\"\n\nclass HelloWindowApplication final : public Application\n{\npublic:\n    HelloWindowApplication(const std::string&amp; title);\n\nprotected:\n    bool Load() override;\n    void Render() override;\n    void Update() override;\n}\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#hellowindowapplicationcpp","title":"HelloWindowApplication.cpp","text":"<pre><code>#include \"HelloWindowApplication.hpp\"\n\nHelloWindowApplication::HelloWindowApplication(const std::string&amp; title)\n    : Application(title)\n{\n}\n\nbool HelloWindowApplication::Load()\n{\n    return true;\n}\n\n\nvoid HelloWindowApplication::Update()\n{\n}\n\nvoid HelloWindowApplication::Render()\n{\n}\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-1-hello-window/#maincpp","title":"Main.cpp","text":"<pre><code>#include \"HelloWindowApplication.hpp\"\n\nint main(int argc, char* argv[])\n{\n    HelloWindowApplication application{ \"LearnD3D11 - Hello Window\" };\n    application.Run();\n}\n</code></pre> <p>Let's start with <code>Main.cpp</code>. That's all its doing, creating the \"hellowindow\" application and running it. In the future this can be accompanied by loading a configuration, initializing a logger, initiating a connection to a possible server, or other stuff.</p> <pre><code>public:\n...\n    void Run();\n...\n</code></pre> <p>This is a section of the <code>Application</code> class, showing only its publicly available methods. <code>Run</code> being the most important one to the outside world, like <code>Main</code>, it's the entry point into this Application.</p> <p>We still don't want to cram everything into one main or one method, therefore <code>Run</code> is split up again into the following blocks.</p> <pre><code>void Application::Run()\n{\n    if (!Initialize())\n    {\n        return;\n    }\n\n    if (!Load())\n    {\n        return;\n    }\n\n    while (!glfwWindowShouldClose(_window))\n    {\n        glfwPollEvents();\n        Update();\n        Render();\n    }\n}\n</code></pre> <p>This function is pretty simple. <code>Initialize</code>, as the name suggests, will initialize everything which is required for the app to run, which currently is the window in our case. In future chapters it will also include initializing <code>D3D11</code>, its resources, and ImGUI for the UI.</p> <p><code>Load</code>'s purpose is to load all the assets required to run the application, in further chapters it will encompass textures, shaders, models and other things.</p> <p>The next block is the aforementioned mainloop or game loop, which still does what it was doing before, checking with the OS if events need to be processed, and now we also call a <code>Update</code> and <code>Render</code> method.</p> <p><code>Update</code> may contain queries about pressed key or mouse buttons, updating variables or other things which are - for instance - reflected on display inside the <code>Render</code> method.</p> <p>You probably have noticed that all the protected method in <code>Application</code> are virtual, that's because we are deriving from <code>Application</code> in form of <code>HelloWindowApplication</code> and only focus on those four methods if required. We now don't have to deal with the mainloop anymore for the time being.</p> <pre><code>virtual void Cleanup();\nvirtual bool Initialize();\nvirtual bool Load() = 0;\nvirtual void Render() = 0;\nvirtual void Update() = 0;\n</code></pre> <p>If you run this example, you will still get the same window as shown below, same behaviour, only the code has been spit up into a more logical piece of work, which will make our life easier as we move on adding more and more.</p> <p></p> <p>Unabstracted Hello Window Project on GitHub</p> <p>Raw Winapi Hello Window Project on GitHub</p> <p>Project on GitHub</p>"},{"location":"1-introduction/1-1-getting-started/1-1-2-hello-d3d11/","title":"Hello D3D11","text":"<p>In this chapter, we'll introduce you to the basics of using D3D11; how to create a ID3D11Device and how to use it to show something in our window. In the last chapter we set up a basic implementation for an application with a window through GLFW. The implementation for <code>Main.cpp</code> and <code>Application.cpp</code> won't be shown here anymore.</p> <p>If you are looking at the source code for this chapter, you will also notice that <code>Application.cpp</code> and <code>Application.hpp</code> do not exist anymore, as we have moved both of these files into a separate <code>Framework</code> project that creates a static library to ease development between chapters. This <code>Framework</code> project will include code that is shared between all chapters, so it might include a lot of other files which are not used or are not relevant within some chapters.</p> <p>The <code>Framework</code> project can be found here.</p> <p>Please note that the code for already existing files is also subject to change to accommodate newer chapters and their needs.</p> <p>However, let's start by breaking down the relevant bits and pieces by showing you how the new class, which derives from <code>Application</code> will look like.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-2-hello-d3d11/#hellod3d11applicationhpp","title":"HelloD3D11Application.hpp","text":"<pre><code>#pragma once\n\n#include &lt;d3d11.h&gt;\n#include &lt;dxgi1_3.h&gt;\n#include &lt;wrl.h&gt;\n\n#include &lt;Application.hpp&gt;\n\nclass HelloD3D11Application final : public Application\n{\n    template &lt;typename T&gt;\n    using ComPtr = Microsoft::WRL::ComPtr&lt;T&gt;;\n\npublic:\n    HelloD3D11Application(const std::string&amp; title);\n    ~HelloD3D11Application() override;\n\nprotected:\n    bool Initialize() override;\n    bool Load() override;\n    void OnResize(\n        int32_t width,\n        int32_t height) override;\n    void Update() override;\n    void Render() override;\n\nprivate:\n    bool CreateSwapchainResources();\n    void DestroySwapchainResources();\n\n    ComPtr&lt;ID3D11Device&gt; _device = nullptr;\n    ComPtr&lt;ID3D11DeviceContext&gt; _deviceContext = nullptr;\n    ComPtr&lt;IDXGIFactory2&gt; _dxgiFactory = nullptr;\n    ComPtr&lt;IDXGISwapChain1&gt; _swapChain = nullptr;\n    ComPtr&lt;ID3D11RenderTargetView&gt; _renderTarget = nullptr;\n};\n</code></pre>"},{"location":"1-introduction/1-1-getting-started/1-1-2-hello-d3d11/#hellod3d11applicationcpp","title":"HelloD3D11Application.cpp","text":"<p>And the implementation side</p> <pre><code>#include \"HelloD3D11Application.hpp\"\n\n#include &lt;GLFW/glfw3.h&gt;\n#define GLFW_EXPOSE_NATIVE_WIN32\n#include &lt;GLFW/glfw3native.h&gt;\n\n#include &lt;DirectXMath.h&gt;\n#include &lt;d3dcompiler.h&gt;\n\n#include &lt;iostream&gt;\n\n#pragma comment(lib, \"d3d11.lib\")\n#pragma comment(lib, \"dxgi.lib\")\n#pragma comment(lib, \"d3dcompiler.lib\")\n#pragma comment(lib, \"winmm.lib\")\n#pragma comment(lib, \"dxguid.lib\")\n\nHelloD3D11Application::HelloD3D11Application(const std::string&amp; title)\n    : Application(title)\n{\n}\n\nHelloD3D11Application::~HelloD3D11Application()\n{\n}\n\nbool HelloD3D11Application::Initialize()\n{\n    return true;\n}\n\nbool HelloD3D11Application::Load()\n{\n    return true;\n}\n\nbool HelloD3D11Application::CreateSwapchainResources()\n{\n    return true;\n}\n\nvoid HelloD3D11Application::DestroySwapchainResources()\n{\n}\n\nvoid HelloD3D11Application::OnResize(\n    const int32_t width,\n    const int32_t height)\n{\n}\n\nvoid HelloD3D11Application::Update()\n{\n}\n\nvoid HelloD3D11Application::Render()\n{\n}\n</code></pre> <p>HelloD3D11Application.hpp <pre><code>#include &lt;d3d11.h&gt;\n#include &lt;dxgi1_3.h&gt;\n#include &lt;wrl.h&gt;\n</code></pre></p> <p>HelloD3D11Application.cpp <pre><code>#include &lt;GLFW/glfw3.h&gt;\n#define GLFW_EXPOSE_NATIVE_WIN32\n#include &lt;GLFW/glfw3native.h&gt;\n\n#include &lt;d3dcompiler.h&gt;\n#include &lt;DirectXMath.h&gt;\n</code></pre></p> <p>We need to include the following headers, here's what each of these headers includes:</p> <ul> <li><code>d3d11.h</code>: The core of D3D11, it contains all the ID3D11XXX types and most of the enums we will be using with D3D11</li> <li><code>dxgi1_3.h</code>: The core of DXGI, it contains all the IDXGIXXX types and additional enums that are required for DXGI structures</li> <li><code>d3dcompiler.h</code>: Contains all the functions necessary to compiler our HLSL shaders into bytecode that will be fed into the GPU</li> <li><code>DirectXMath.h</code>: DirectX's own math library, it contains all the types and math functions we will be using throughout the series</li> <li><code>wrl.h</code>: Is used for <code>Microsoft::WRL::ComPtr&lt;T&gt;</code>, to manage COM resources automatically.</li> </ul> <pre><code>#pragma comment(lib, \"d3d11.lib\")\n#pragma comment(lib, \"dxgi.lib\")\n#pragma comment(lib, \"d3dcompiler.lib\")\n#pragma comment(lib, \"winmm.lib\")\n#pragma comment(lib, \"dxguid.lib\")\n</code></pre> <p>Of course just including the headers isn't enough, we must also link against D3D11 &amp; friends to be able to actually use the stuff declared in the headers, put these <code>#pragma comment(lib, \"PATH_TO_LIB\")</code> in <code>HelloD3D11Application.cpp</code> right below the includes to link these libraries. </p> <pre><code>ComPtr&lt;IDXGIFactory2&gt; _dxgiFactory = nullptr;\nComPtr&lt;ID3D11Device&gt; _device = nullptr;\nComPtr&lt;ID3D11DeviceContext&gt; _deviceContext = nullptr;\nComPtr&lt;IDXGISwapChain1&gt; _swapChain = nullptr;\nComPtr&lt;ID3D11RenderTargetView&gt; _renderTarget = nullptr;\n</code></pre> <p>You might have noticed that we are not using raw pointers for those pieces, but <code>ComPtr</code>. DirectX is built on top of COM (Component Object Model) and with that COM objects utilize reference counting to manage object lifetimes, in form of <code>AddRef</code> and <code>Release</code> methods. <code>ComPtr&lt;T&gt;</code> wraps that functionlity for us, by creating a smart pointer. You can find more information about it here.</p> <p><code>IDXGIFactory2</code> helps us find an adapter we can use to run our graphics on. It can enumerate all existing adapters (GPUs), of which there could be several installed in your system. If you have a laptop there is most likely an integrated one coming with your cpu, but often these days laptops also have a dedicated GPU as well, or your PC might have more than one dedicated GPUs installed. With <code>IDXGIFactory2</code> we can pick one. It also creates the swapchain for us, a surface to store rendered data before presenting it to an output (or screen).</p> <p><code>ID3D11Device</code> is the object which we use to create all sorts of things, buffers, textures, samplers, shaders.</p> <p><code>ID3D11DeviceContext</code> is the one we use to issue draw and compute commands to the GPU.</p> <p><code>IDXGISwapChain1</code> The aforementioned surface, which stores rendered data which it can present to an output (or screen).</p> <p><code>ID3D11RenderTargetView</code> Is a fancy pointer to a texture, this tells D3D11 that the texture this points to, is drawable within the subresource of the referenced texture</p> <p><code>DXGI</code> stands for DirectX Graphics Infrastructure, in case you are wondering.</p> <p>Let's go in to <code>Initialize</code></p> <pre><code>if (!Application::Initialize())\n{\n    return false;\n}\n\nif (FAILED(CreateDXGIFactory1(IID_PPV_ARGS(&amp;_dxgiFactory))))\n{\n    std::cout &lt;&lt; \"DXGI: Unable to create DXGIFactory\\n\";\n    return false;\n}\n</code></pre> <p>The first part calls the parent class, where <code>GLFW</code> is initialized and setup.</p> <p><code>IID_PPV_ARGS(ppType)</code> Is a compile-time macro that is defined as  <pre><code>#define IID_PPV_ARGS(ppType) __uuidof(**(ppType)), IID_PPV_ARGS_Helper(ppType)\n</code></pre> Which means that typing <code>IID_PPV_ARGS(&amp;_dxgiFactory)</code> it is expanded by the compiler into <code>__uuidof(**(&amp;_dxgiFactory)), IID_PPV_ARGS_Helper(_dxgiFactory)</code>. This functionally means that for functions that have a parameter setup as <code>REFIID</code> and functionally after a <code>[out] void**</code> parameter, this macro will expand the <code>IID_PPV_ARGS(ppType)</code> expression into these parameters for ease of use \u2014 this can be seen with the used <code>CreateDXGIFactory1</code> method where the parameters are a <code>REFIID</code> and <code>void**</code>: <pre><code>HRESULT CreateDXGIFactory1(\n        REFIID riid,\n  [out] void   **ppFactory\n);\n</code></pre> <code>REFIID</code> is a typedef that is a Reference (REF) to an Interface Identifier type (<code>IID</code>) \u2014 this means that it is a reference to a type that uniquely identifies a COM object. [more information like underlying memory organization can be read about IID's at https://docs.microsoft.com/en-us/office/client-developer/outlook/mapi/iid]</p> <p>What the parts of the <code>IID_PPV_ARGS(ppType)</code> macro are: </p> <p>[the <code>ppType</code> in <code>IID_PPV_ARGS(ppType)</code>] - a pointer to a pointer of an object.</p> <p>[the <code>__uuidof(**(ppType))</code> part of <code>IID_PPV_ARGS(ppType)</code>] -  at compile time retrieves a <code>UUID</code> from <code>ppType</code> type which represents a <code>GUID</code>, which is returned as a <code>REFIID</code> \u2014 which means that the type returned is a reference to an identifier to a specific type of COM object.   </p> <p>Explain DXGI</p> <p>https://docs.microsoft.com/en-us/windows/win32/direct3ddxgi/d3d10-graphics-programming-guide-dxgi</p> <p><code>CreateDXGIFactory1</code> is the entry point to create a factory for us, a <code>IDXGIFactory1</code> to be precise. There are various implementations of it, depending on what version you aim for, you get additional functionality.</p> <p>DXGI 1.0 up to 1.6 More information can be found here We will stick with <code>IDXGIFactory1</code> for now.</p> <pre><code>constexpr D3D_FEATURE_LEVEL deviceFeatureLevel = D3D_FEATURE_LEVEL::D3D_FEATURE_LEVEL_11_0;\nif (FAILED(D3D11CreateDevice(\n    nullptr,\n    D3D_DRIVER_TYPE::D3D_DRIVER_TYPE_HARDWARE,\n    nullptr,\n    0,\n    &amp;deviceFeatureLevel,\n    1,\n    D3D11_SDK_VERSION,\n    &amp;_device,\n    nullptr,\n    &amp;_deviceContext)))\n{\n    std::cout &lt;&lt; \"D3D11: Failed to create device and device Context\\n\";\n    return false;\n}\n</code></pre> <p>This block is the entry point into D3D11, where we ask for a device and its device context to be created. The input parameters are:</p> <p>We want a LEVEL_11_0, hardware accelerated device, which has support for a specific color format. Feature levels are a concept that has been introduced with D3D11, it is a way to specify which set of features we would like to use. Each GPU may support different feature levels (for example a very old GPU might only support LEVEL_9_1, while a more modern one may support every feature level up to, and including LEVEL_11_0), this is a way to avoid rewriting our application in D3D9 just because our GPU doesn't support D3D11.</p> <p>If <code>D3D11CreateDevice</code> succeeds we will get a <code>ID3D11Device</code> and a <code>ID3D11DeviceContext</code> back.</p> <pre><code>DXGI_SWAP_CHAIN_DESC1 swapChainDescriptor = {};\nswapChainDescriptor.Width = GetWindowWidth();\nswapChainDescriptor.Height = GetWindowHeight();\nswapChainDescriptor.Format = DXGI_FORMAT::DXGI_FORMAT_B8G8R8A8_UNORM;\nswapChainDescriptor.SampleDesc.Count = 1;\nswapChainDescriptor.SampleDesc.Quality = 0;\nswapChainDescriptor.BufferUsage = DXGI_USAGE_RENDER_TARGET_OUTPUT;\nswapChainDescriptor.BufferCount = 2;\nswapChainDescriptor.SwapEffect = DXGI_SWAP_EFFECT::DXGI_SWAP_EFFECT_FLIP_DISCARD;\nswapChainDescriptor.Scaling = DXGI_SCALING::DXGI_SCALING_STRETCH;\nswapChainDescriptor.Flags = {};\n\nDXGI_SWAP_CHAIN_FULLSCREEN_DESC swapChainFullscreenDescriptor = {};\nswapChainFullscreenDescriptor.Windowed = true;\n\nif (FAILED(_dxgiFactory-&gt;CreateSwapChainForHwnd(\n    _device.Get(),\n    glfwGetWin32Window(GetWindow()),\n    &amp;swapChainDescriptor,\n    &amp;swapChainFullscreenDescriptor,\n    nullptr,\n    &amp;_swapChain)))\n{\n    std::cout &lt;&lt; \"DXGI: Failed to create swapchain\\n\";\n    return false;\n}\n</code></pre> <p>After we successfully create device and device context, the next step is to create a swapchain, that storage containing the rendered images which we can present to the screen.</p> <p>The majority of values should make some sense without explanation, like width and height, and whether we want it to support a windowed window or not.</p> <p><code>BufferUsage</code> tells the swapchain's buffers their usage, something we render to, and can present.</p> <p><code>Scaling</code> tells DXGI how to scale the buffer's contents to fit the presentation's target size.</p> <p><code>BufferCount</code> is 2, because we want double buffering. Double buffering is an age-old technique to avoid presenting an image that is being used by the GPU, instead we work on the \"back buffer\", while the GPU is happy presenting the \"front buffer\", then, as soon as we are done with the back buffer, we swap front and back, and begin working on the former front buffer present that one and render to the other one again in the meantime. That process is supposed to reduce flicker or tearing.</p> <p><code>SwapEffect</code> specifies if the contents of the back buffer should be preserved or discarded after a swap, here we don't care about preserving the back buffer, so we just discard everything.</p> <p><code>AlphaMode</code> specifies how DXGI should handle transparency, we don't care about that (yet), so we'll just say it's unspecified and rely on default behaviour</p> <pre><code>if (!CreateSwapchainResources())\n{\n    return false;\n}\n\nreturn true;\n</code></pre> <p>And the last bits of the <code>Initialize</code> method.</p> <p>We need to create a few more things. Those are based on the swapchain, hence their name. These resources need to be destroyed and recreated whenever we want to resize the window. When that happens, the swapchain needs to be resized as well (since that is a prameter in its descriptor as you can see above)</p> <pre><code>bool HelloD3D11Application::CreateSwapchainResources()\n{\n    ComPtr&lt;ID3D11Texture2D&gt; backBuffer = nullptr;\n    if (FAILED(_swapChain-&gt;GetBuffer(\n        0,\n        IID_PPV_ARGS(&amp;backBuffer))))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to get Back Buffer from the SwapChain\\n\";\n        return false;\n    }\n\n    if (FAILED(_device-&gt;CreateRenderTargetView(\n        backBuffer.Get(),\n        nullptr,\n        &amp;_renderTarget)))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to create RTV from Back Buffer\\n\";\n        return false;\n    }\n\n    return true;\n}\n</code></pre> <p>When we render things, the GPU simply writes color values to a texture, which you can picture as a buffer which holds color information Swapchain is a container to manage those buffers we want to present on screen. To do that we have to create a special kind of texture called a \"Render Target View\" or an RTV. First off we have to grab a texture from the swapchain's main buffer (index 0), from that texture, we now have to create an RTV from that, which specifies the subresource of the texture that we will be drawing to. We won't keep the actual texture around, we just need the render target view, which we will refer to as render target.</p> <pre><code>void HelloD3D11Application::DestroySwapchainResources()\n{\n    _renderTarget.Reset();\n}\n</code></pre> <p>The render target needs to be disposed when we want to resize (or cleanup in general), it will be recreated via <code>CreateSwapchainResources</code> when we resize the window as shown here:</p> <pre><code>void HelloD3D11Application::OnResize(\n    const int32_t width,\n    const int32_t height)\n{\n    Application::OnResize(width, height);\n    _deviceContext-&gt;Flush();\n\n    DestroySwapchainResources();\n\n    if (FAILED(_swapChain-&gt;ResizeBuffers(\n        0,\n        width,\n        height,\n        DXGI_FORMAT::DXGI_FORMAT_B8G8R8A8_UNORM,\n        0)))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to recreate SwapChain buffers\\n\";\n        return;\n    }\n\n    CreateSwapchainResources();\n}\n</code></pre> <p>When we resize, let the base application know about it, and make sure the device context has done all its work (<code>Flush</code>)</p> <p>Before we can resize the swapchain, make sure all resources based on it are disposed. Afterwards recreate them with the new dimensions of the swapchain</p> <pre><code>void HelloD3D11Application::Render()\n{\n    D3D11_VIEWPORT viewport = {};\n    viewport.TopLeftX = 0;\n    viewport.TopLeftY = 0;\n    viewport.Width = static_cast&lt;float&gt;(GetWindowWidth());\n    viewport.Height = static_cast&lt;float&gt;(GetWindowHeight());\n    viewport.MinDepth = 0.0f;\n    viewport.MaxDepth = 1.0f;\n\n    constexpr float clearColor[] = { 0.1f, 0.1f, 0.1f, 1.0f };\n\n    _deviceContext-&gt;ClearRenderTargetView(\n        _renderTarget.Get(),\n        clearColor);\n    _deviceContext-&gt;RSSetViewports(\n        1,\n        &amp;viewport);\n    _deviceContext-&gt;OMSetRenderTargets(\n        1,\n        _renderTarget.GetAddressOf(),\n        nullptr);\n    _swapChain-&gt;Present(1, 0);\n}\n</code></pre> <p>Now we can actually use those things we have created before. We just set it up so that we tell <code>D3D11</code> that we want to render into the render target, and when we clear we want to use a dark gray.</p> <p>We also have to specify an area in form of a rectangle, in this case, its equivalent to the window size.</p> <p>Last but not least, we Present the content of the swapchain to the window, using <code>Present</code>. The first argument defines which vblanks to synchronize with presentation, 0 means: no synchronization (unlimited FPS), 1 means: sync every v-blank (regular v-sync), 2 means: sync every other v-blank and so on, up to 4. The second are optional flags, we don't need them so 0 is passed.</p> <p><code>Application</code> also defines an abstract method <code>Update</code> which we have to define here as well, so we will add:</p> <pre><code>void HelloD3D11Application::Update()\n{\n}\n</code></pre> <p>But keep it empty for now.</p> <p>Same applies for <code>Application</code>'s <code>Load</code> method.</p> <pre><code>bool HelloD3D11Application::Load()\n{\n    return true;\n}\n</code></pre> <p>Finally, we need to modify <code>Appplication.hpp</code> and <code>Application.cpp</code>. Since we want to handle resizing as well.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-2-hello-d3d11/#applicationhpp","title":"Application.hpp","text":"<p>Find <code>protected:</code> and add the following lines</p> <pre><code>static void HandleResize(\n    GLFWwindow* window,\n    const int32_t width,\n    const int32_t height);\nvirtual void OnResize(\n    const int32_t width,\n    const int32_t height);\n\n[[nodiscard]] GLFWwindow* GetWindow() const;\n[[nodiscard]] int32_t GetWindowWidth() const;\n[[nodiscard]] int32_t GetWindowHeight() const;\n</code></pre> <p><code>HandleResize</code> will be the callback from <code>GLFW</code> which handles resize events and <code>OnResize</code> will be executed when GLFW runs HandleResize, so that we can handle our custom things we want to execute when resizing the window, like changing the size of the swapchain in our example.</p> <p><code>GetWindow()</code> is used to derive the actual native window handle from, which is needed when we create the swapchain. <code>GetWindowWidth()</code> and <code>GetWindowHeight()</code> do what they say :) Also required for swapchain creation.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-2-hello-d3d11/#applicationcpp","title":"Application.cpp","text":"<p>Add the following lines</p> <pre><code>void Application::OnResize(\n    const int32_t width,\n    const int32_t height)\n{\n    _width = width;\n    _height = height;\n}\n\nvoid Application::HandleResize(\n    GLFWwindow* window,\n    const int32_t width,\n    const int32_t height)\n{\n    Application* application = static_cast&lt;Application*&gt;(glfwGetWindowUserPointer(window));\n    application-&gt;OnResize(width, height);\n}\n\nGLFWwindow* Application::GetWindow() const\n{\n    return _window;\n}\n\nint32_t Application::GetWindowWidth() const\n{\n    return _width;\n}\n\nint32_t Application::GetWindowHeight() const\n{\n    return _height;\n}\n</code></pre> <p>Find the <code>Initialize</code> method and add the following two lines before <code>return true;</code></p> <pre><code>glfwSetWindowUserPointer(_window, this);\nglfwSetFramebufferSizeCallback(_window, HandleResize);\n</code></pre> <p><code>glfwSetWindowUserPointer</code> will set our application instance as a custom variable, so that we can retrieve it using <code>glfwGetWindowUserPointer</code> in the <code>HandleResize</code> callback.</p> <p><code>glfwSetFramebufferSizeCallback</code> will tell <code>GLFW</code> what to do when we resize the window, in this case execute <code>HandleResize</code> which will fetch our application instance and all <code>OnResize</code> on it, where we can handle resizing in our application code.</p> <p>Project on GitHub</p> <p>Next chapter</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/","title":"Hello Triangle","text":"<p>In the last chapter we initialized core components of <code>D3D11</code> and DXGI such as  the Device and the SwapChain, but simply clearing the Window with  some color is pretty boring.</p> <p>This time we will be drawing our first triangle with a nice froge-like color.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#the-pipeline","title":"The Pipeline","text":"<p>The fundamental part of all graphic APIs is the \"Graphics Pipeline\". Everything  from a single triangle, textured frog or the whole Elden Ring map goes through  this pipeline. It is a series of functions that either exist in hardware, can  be configured or are fully programmable. It transforms everything we draw in  3D space to the 2D space that is our monitor.</p> <p>All the steps in the graphics pipeline go from top to bottom and are shown below.</p> <p></p> <p>As you can see, each stage in the pipeline takes in the previous stage's output  as the input, the rectangle blocks are pipeline stages  that are not programmable but are configurable, while the rounded rectangle blocks are stages that are fully programmable.  To draw most of the things throughout this series we will mostly need these stages:  Input Assembler, Vertex Shader and the Pixel Shader, Output Merger.</p> <p>The Vertex and Pixel shaders are fully programmable and we will write a very basic  program for them.</p> <p>The other two stages are not programmable but they are fairly easy to understand  and configure:</p> <ul> <li> <p>the Input Assembler is responsible for processing the vertices in an eventual  vertex buffer into the primitive topology of our choice, which in our case is a form  of triangles, and sending this processed output to be processed again - but this time  by our Vertex Shader. </p> </li> <li> <p>The Output Merger is responsible for combining the values written by the  pixel shader, may that be depth, color or other things, into the one or more render  targets that we provide to the OM, we only have one  render target for now.</p> </li> </ul>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#vertex-shader","title":"Vertex Shader","text":"<p>The Vertex Shader is the stage where our vertices are  processed however we want</p> <p>The vertices are usually read from a Vertex Buffer \u2014 usually  since there are no hard rules in programming \u2014 we can always be inventive to create or  read data differently on the fly! This all works since the vertex shader will be run however many times we tell  it to run, which is specified in the first parameter of <code>ID3D11DeviceContext::Draw()</code>  (more on this later), for instance if we call <code>Draw(3, 0)</code>, the vertex shader will  run 3 times.</p> <p>An example of unique methods to acquire vertex data is with how the  vertex buffer can be omitted, if we want to draw a full screen triangle by just hardcoding  vertices  in our vertex shader \u2014 removing  the need of a vertex buffer.</p> <p>Since we only want to draw a triangle, we do not need to do much processing in our vertex shader code, we can just provide the input vertices as the output.</p> <p>Let's look at our basic vertex shader for this section:</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#mainvshlsl","title":"Main.vs.hlsl","text":"<pre><code>struct VSInput\n{\n    float3 position: POSITION;\n    float3 color: COLOR0;\n};\n\nstruct VSOutput\n{\n    float4 position: SV_Position;\n    float3 color: COLOR0;\n};\n\nVSOutput Main(VSInput input)\n{\n    VSOutput output = (VSOutput)0;\n    output.position = float4(input.position, 1.0);\n    output.color = input.color;\n    return output;\n}\n</code></pre> <p>First off, we define 2 types, <code>VSInput</code> and <code>VSOutput</code> which represent the  vertex shader's input and output.</p> <p>There are two inputs, both of them are a float3.  float3 is a type that holds 3 floating point numbers.</p> <p>The first input is the position field (x y z as the 3 floats) the second one is the color field (r g b as the 3 floats).</p> <p>We will send both of these over to the output of this  stage, onto the pixel shader.</p> <p>Notice how all our fields have a colon and some identifier attached to them,  these are \"semantics\". Semantics that are preceded by <code>SV</code> are called  \"system-value semantics\" and their meaning and usage is defined by D3D11.  <code>SV_Position</code> for example means that the field <code>position</code> will be used by  D3D11 as the actual output of the vertex shader.</p> <p>Everything else are \"user defined semantics\" and their naming is up to us.  These are used to pass data between shader stages.</p> <p>Then we have our <code>VSOutput</code>, which has our vertices in the first field <code>position</code>  and our color in the second field <code>color</code>.</p> <p>if the <code>SV_Position</code> values are tried to be given a position outside the range of  [-1.0, 1.0] they are clipped and we will not see them on  screen. Due to this we need to create our own math to transform any coordinates we  have to a normalized [-1.0, 1.0] This is going to be explored later in more depth under the name of Normalized Device Coordinates (NDC).</p> <p>Finally, we have our main function, which in our case  takes in a single parameter which is  our input in the form of a <code>VSInput</code> struct, and returns  our output in the form of a <code>VSOutput</code>.  Since we do not do any processing, we simply make a new   instance of <code>VSOutput</code>,  initialize it all to 0 and   forward our input position and color to the output.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#pixel-shader","title":"Pixel Shader","text":"<p>The Pixel Shader is the stage (by default) where we set the pixels  on our render target, it is invoked for each pixel that is  covered by a triangle formed by previous shader(s)</p> <p>We use this (Pixel Shader) stage generally to apply most  of our shading techniques, from anything such as basic  lighting, to textures, and more.</p> <p>Since we did not specify any shader between the VS and the  PS, our input here is the output of the VS</p> <p>Let's look at our Pixel Shader now:</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#mainpshlsl","title":"Main.ps.hlsl","text":"<pre><code>struct PSInput\n{\n    float4 position: SV_Position;\n    float3 color: COLOR0;\n};\n\nstruct PSOutput\n{\n    float4 color: SV_Target0;\n};\n\nPSOutput Main(PSInput input)\n{\n    PSOutput output = (PSOutput)0;\n    output.color = float4(input.color, 1.0);\n    return output;\n}\n</code></pre> <p>Here as well we have an input <code>PSInput</code>, and an output <code>PSOutput</code>.</p> <p>Since we have not setup any other shaders in between the VS and the PS, the VS's  output is the PS's input, the naming might be a bit confusing but that's the  gist of it, PSInput should match the VSOutput in vertex shader, this isn't  entirely required but not doing so is only advisable if you really know what  you are doing.</p> <p>Next we have our output, <code>D3D11</code> gives us the possibility to write to multiple  render targets, but we are not doing that, so we will just be writing a <code>float4</code>  as our output, which is an RGBA color.</p> <p>Notice how we have another semantic string attached to the <code>color</code> field,  this semantic string specifies which render target we want to be writing to,  the <code>0</code> after <code>SV_Target</code> is the index of our render target, in our case,  we have only one, so we write <code>SV_Target0</code> or <code>SV_Target</code>.</p> <p><code>D3D11</code> lets us write up to 8 render targets simultaneously from the same pixel  shader, those come in handy when implementing more advanced shading techniques.</p> <p>And lastly, our <code>Main</code> function.  Following the same pattern as in the VS we have already:  one input (<code>VSInput</code>) and output parameter (<code>PSOutput</code>).</p> <p>we initialize <code>PSOutput</code> and set everything to 0 to then   write the color we got from the input to our output.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#compiling-shaders","title":"Compiling shaders","text":"<p>Now that we wrote our shader code and saved it somewhere, we have to feed this </p> <p>to the GPU, to do that we will use our D3DCompiler to compile!</p> <p>First, we will declare some functions that will help us compile our shaders more quickly.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#hellotrianglehpp","title":"HelloTriangle.hpp","text":"<pre><code>bool CompileShader(\n    const std::wstring&amp; fileName,\n    const std::string&amp; entryPoint,\n    const std::string&amp; profile,\n    ComPtr&lt;ID3DBlob&gt;&amp; shaderBlob) const;\n\n[[nodiscard]] ComPtr&lt;ID3D11VertexShader&gt; CreateVertexShader(\n    const std::wstring&amp; fileName,\n    ComPtr&lt;ID3DBlob&gt;&amp; vertexShaderBlob) const;\n\n[[nodiscard]] ComPtr&lt;ID3D11PixelShader&gt; CreatePixelShader(std::wstring&amp; fileName) const;\n</code></pre> <p>In order, we have:</p> <p><code>CompileShader</code>: This function is the core for compiling shaders, it requires 3  input parameters:</p> <ul> <li><code>fileName</code>: is the path of the shader file we want to compile.</li> <li><code>entryPoint</code>: is the name of the function where the shader begins execution.</li> <li><code>profile</code>: which is basically the version of HLSL we want to use, the higher the               profile number, the more features there are.</li> </ul> <p>And one output parameter:</p> <ul> <li><code>shaderBlob</code>: the blob were our compiled code will be stored. A blob is just    a fancy buffer which <code>D3D11</code> can use for specific purposes.</li> </ul> <p>Then: <code>CreateVertexShader</code>: This function helps us create specifically a  <code>ID3D11VertexShader</code>, it only requires the shader path and a  <code>ID3DBlob</code>.</p> <p>We need to pass a blob ourselves because we will need the VS's blob later.</p> <p><code>CreatePixelShader</code>: It does the same thing that <code>CreateVertexShader</code>  does, except we do not need to pass a <code>ID3DBlob</code> here.</p> <p>Now that we know how our new members look, we will see how we implemented them.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#hellotrianglecpp","title":"HelloTriangle.cpp","text":"<p>First things first, let's see <code>CompileShader</code>:</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#compileshader","title":"CompileShader","text":"<pre><code>bool HelloTriangleApplication::CompileShader(\n    const std::wstring&amp; fileName,\n    const std::string&amp; entryPoint,\n    const std::string&amp; profile,\n    ComPtr&lt;ID3DBlob&gt;&amp; shaderBlob) const\n{\n    constexpr UINT compileFlags = D3DCOMPILE_ENABLE_STRICTNESS;\n\n    ComPtr&lt;ID3DBlob&gt; tempShaderBlob = nullptr;\n    ComPtr&lt;ID3DBlob&gt; errorBlob = nullptr;\n    if (FAILED(D3DCompileFromFile(\n        fileName.data(),\n        nullptr,\n        D3D_COMPILE_STANDARD_FILE_INCLUDE,\n        entryPoint.data(),\n        profile.data(),\n        compileFlags,\n        0,\n        &amp;tempShaderBlob,\n        &amp;errorBlob)))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to read shader from file\\n\";\n        if (errorBlob != nullptr)\n        {\n            std::cout &lt;&lt; \"D3D11: With message: \" &lt;&lt; \n            static_cast&lt;const char*&gt;(errorBlob-&gt;GetBufferPointer()) &lt;&lt; \"\\n\";\n        }\n\n        return false;\n    }\n\n    shaderBlob = std::move(tempShaderBlob);\n    return true;\n}\n</code></pre> <p>We start by creating two <code>ID3DBlob</code>s, we will need a temporary blob, where we will load our shader file and an error blob, which will contain our error messages, if any.</p> <p>Then we call for <code>D3DCompileFromFile</code>,  it requires quite a lot of parameters so let's go over them one by one in order:</p> <ul> <li><code>pFileName</code>: a UTF-8 string containing the file name of the shader we want to compile.</li> <li><code>pDefines</code>: optional, basically an array of macros that we want to define.</li> <li><code>pInclude</code>: optional, a pointer to a <code>ID3DInclude</code> object, it is useful to     specify how to handle <code>#include</code> directives in shaders. It is common to     just use <code>D3D_COMPILE_STANDARD_FILE_INCLUDE</code>, which is the default handler.</li> <li><code>pEntrypoint</code>: a string containing the name of the main function in the shader -  Defaults to <code>main</code> if NULL</li> <li><code>pTarget</code>: a string containing the Shader Model version to use for this shader.</li> <li><code>Flags1</code>: the flags that changes how to compile our shaders, for example we pass   <code>D3DCOMPILE_ENABLE_STRICTNESS</code> which makes the compiler stricter in judging our    code and disables legacy syntax support.</li> <li><code>Flags2</code>: ignored, set to 0.</li> <li><code>ppCode</code>: output, a pointer to a <code>ID3DBlob*</code>, this is where our compiled code will    be stored.</li> <li><code>ppErrorMsgs</code>: optional, output, a pointer to a <code>ID3DBlob*</code>, this is where the D3D    compiler will store our errors, <code>nullptr</code> if everything went fine.</li> </ul> <p>Then we do our usual checking, if there were errors, leave the output blob  as is and print the error message contained in the blob. Otherwise, move  the blob to our output parameter.</p> <p>Now let's see <code>CreateVertexShader</code> and <code>CreatePixelShader</code>:</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#createvertexshader","title":"CreateVertexShader","text":"<pre><code>HelloTriangleApplication::ComPtr&lt;ID3D11VertexShader&gt; HelloTriangleApplication::CreateVertexShader(\n    const std::wstring&amp; fileName,\n    ComPtr&lt;ID3DBlob&gt;&amp; vertexShaderBlob) const\n{\n    if (!CompileShader(fileName, \"Main\", \"vs_5_0\", vertexShaderBlob))\n    {\n        return nullptr;\n    }\n\n    ComPtr&lt;ID3D11VertexShader&gt; vertexShader;\n    if (FAILED(_device-&gt;CreateVertexShader(\n        vertexShaderBlob-&gt;GetBufferPointer(),\n        vertexShaderBlob-&gt;GetBufferSize(),\n        nullptr,\n        &amp;vertexShader)))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to compile vertex shader\\n\";\n        return nullptr;\n    }\n\n    return vertexShader;\n}\n</code></pre> <p>As you can see here we are using our helper function <code>CompileShader</code> to avoid repeating ourselves, we are specifying <code>\"Main\"</code> as the entry point of our  vertex shader and <code>\"vs_5_0\"</code> as the Shader Model, which means  \"Vertex Shader Model 5.0\".</p> <p>After we get our blob successfully, we can create a vertex shader out of it with <code>ID3D11Device::CreateVertexShader</code>, it takes a pointer to a buffer with the compiled  code and its size as the input. The resulting vertex shader is the last parameter  which is our output.</p> <p>And finally</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#createpixelshader","title":"CreatePixelShader","text":"<pre><code>HelloTriangleApplication::ComPtr&lt;ID3D11PixelShader&gt; \nHelloTriangleApplication::CreatePixelShader(const std::wstring&amp; fileName) const\n{\n    ComPtr&lt;ID3DBlob&gt; pixelShaderBlob = nullptr;\n    if (!CompileShader(fileName, \"Main\", \"ps_5_0\", pixelShaderBlob))\n    {\n        return nullptr;\n    }\n\n    ComPtr&lt;ID3D11PixelShader&gt; pixelShader;\n    if (FAILED(_device-&gt;CreatePixelShader(\n        pixelShaderBlob-&gt;GetBufferPointer(),\n        pixelShaderBlob-&gt;GetBufferSize(),\n        nullptr,\n        &amp;pixelShader)))\n    {\n        std::cout &lt;&lt; \"D3D11: Failed to compile pixel shader\\n\";\n        return nullptr;\n    }\n\n    return pixelShader;\n}\n</code></pre> <p>Pretty much the same thing as <code>CreateVertexShader</code>, the only thing that changes is the <code>profile</code> parameter from <code>\"vs_5_0\"</code> to <code>\"ps_5_0\"</code>, since we are not compiling a  vertex shader now, we have to change this to the \"Pixel Shader Model 5.0\".</p> <p>After all of this, we can now call these functions, in  <code>HelloTriangleApplication::Initialize()</code> you should now add:</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#initialize","title":"Initialize","text":"<pre><code>ComPtr&lt;ID3DBlob&gt; vertexShaderBlob = nullptr;\n_vertexShader = CreateVertexShader(L\"Assets/Shaders/Main.vs.hlsl\", vertexShaderBlob);\nif (_vertexShader == nullptr)\n{\n    return false;\n}\n\n_pixelShader = CreatePixelShader(L\"Assets/Shaders/Main.ps.hlsl\");\nif (_pixelShader == nullptr)\n{\n    return false;\n}\n</code></pre> <p>We still have a <code>vertexShaderBlob</code> now, it will be useful to us later, in creating an  input layout.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#input-layouts","title":"Input Layouts","text":"<p>We have successfully compiled our shaders now, we need one last thing,  an Input Layout. An input layout, is basically the format we want  to lay our vertices in our buffers.</p> <p>Since all our vertices we want to give to the GPU must be tightly packed in the  same buffer, the Input Assembler needs a way to make sense of our data, this is  exactly what an input layout is for - it tells the GPU exactly how the memory in  the buffer will be organized, and how it should be mapped to our expected vertex  shader, vertex layout (<code>VSInput</code> in our case)</p> <p>Let's see what input we expect in the vertex shader again:</p> <pre><code>struct VSInput\n{\n    float3 position: POSITION;\n    float3 color: COLOR0;\n};\n</code></pre> <p>The vertex shader expects per vertex: two vectors of 3 (4 byte) components. </p> <p>We should then create an input layout exactly with this format:</p> <p>First of all, creating a <code>struct</code> in our C++ source with the same field layout  as our <code>VSInput</code> will make our life easier when imagining how 1 vertex will fit  on the GPU one after each other.</p> <p>To do this we will use <code>DirectXMath</code> which has types that map perfectly to HLSL,  both of our inputs are <code>float3</code> in HLSL, which means that this translates  to <code>DirectX::XMFLOAT3</code> </p> <p><code>DirectX::XMFLOAT3</code> wraps 3 floats like an array, it is a class designed to work  with <code>DirectXMath</code></p> <pre><code>using Position = DirectX::XMFLOAT3;\nusing Color = DirectX::XMFLOAT3;\n\nstruct VertexPositionColor\n{\n    Position position;\n    Color color;\n};\n</code></pre> <p>The type aliases (<code>using Position</code> and <code>using Color</code>) help us make this code more  readable to easily guess what is what type. The first field is our position vector  and the second field is our color vector \u2014 notice it is exactly like our <code>VSInput</code>.</p> <p>Now we can create our Input Layout Description using  an array of <code>D3D11_INPUT_ELEMENT_DESC</code>. </p> <pre><code>constexpr D3D11_INPUT_ELEMENT_DESC vertexInputLayoutInfo[] =\n{\n    {\n        \"POSITION\",\n        0,\n        DXGI_FORMAT::DXGI_FORMAT_R32G32B32_FLOAT,\n        0,\n        offsetof(VertexPositionColor, position),\n        D3D11_INPUT_CLASSIFICATION::D3D11_INPUT_PER_VERTEX_DATA,\n        0\n    },\n    {\n        \"COLOR\",\n        0,\n        DXGI_FORMAT::DXGI_FORMAT_R32G32B32_FLOAT,\n        0,\n        offsetof(VertexPositionColor, color),\n        D3D11_INPUT_CLASSIFICATION::D3D11_INPUT_PER_VERTEX_DATA,\n        0\n    },\n};\n</code></pre> <p>Now let's make sense of why we have the following layout: (we will refer to 1 element of D3D11_INPUT_ELEMENT_DESC as a field)</p> <pre><code>{\n    `SemanticName`,\n    `SemanticIndex`,\n    `Format`,\n    `InputSlot`,\n    `AlignedByteOffset`,\n    `InputSlotClass`,\n    `InstanceDataStepRate`\n},\n</code></pre> <ul> <li> <p><code>SemanticName</code>: let's us refer to a particular field name (the string) after the  colon in HLSL (recall <code>POSITION</code> inside <code>float3 position: POSITION</code>)</p> </li> <li> <p><code>SemanticIndex</code>: the index of each semantic, <code>POSITION</code> is equivalent to <code>POSITION0</code>, where the number at the end is our semantic index, so we will just pass in 0. </p> <p><code>POSITION1</code> in HLSL would have a semantic index of 1, etc.</p> </li> <li> <p><code>Format</code>: the format of this field, basically how many components there are and  what type they are, a <code>float3</code> in HLSL is a vector of 3 floats, each float is 4  bytes wide (or 32 bits), so the format here is <code>DXGI_FORMAT_R32G32B32_FLOAT</code>. DXGI_FORMAT chose R32 G32 B32 as an arbitrary component  name, this has nothing to do with colors, we can store  positions here since the type is <code>FLOAT</code></p> </li> <li> <p><code>InputSlot</code>: we will see about this later.</p> </li> <li><code>AlignedByteOffset</code>: the offset of this field, in bytes</li> <li><code>InputSlotClass</code>: The rate of input is either per-vertex or per-instance, we do not  use instances right now since we are only drawing a triangle so we will set this to PER_VERTEX, and explain PER_INSTANCE in later lessons.</li> <li><code>InstanceDataStepRate</code>: this will be explained with PER_INSTANCE in later lessons,  so for now this value is 0</li> </ul> <p>Each element being sent to the GPU needs to be described on how they are laid out,  therefore we have D3D11_INPUT_ELEMENT_DESC vertexInputLayoutInfo[] to describe the data layout.</p> <p>You can think of each element in this array as describing one element in <code>VSInput</code> </p> <ul> <li><code>POSITION</code> is the first element (offset of 0). <code>POSITION</code> is also a float3 (4+4+4 = 12 bytes). Therefore the GPU expects the first 12 bytes of every vertex to be a float3 filled  with <code>POSITION</code> data.</li> <li><code>COLOR</code> is after <code>POSITION</code>, meaning COLOR has an offset of 12 bytes. because <code>COLOR</code> is also a float3, it is also 12 bytes.</li> </ul> <p>Therefore the GPU expects after the <code>POSITION</code> data, 12 bytes of <code>float3</code> <code>COLOR</code> data.</p> <p>Hopefully it makes a bit more sense now, all we have to do is create the input layout  using this data:</p> <pre><code>if (FAILED(_device-&gt;CreateInputLayout(\n    vertexInputLayoutInfo,\n    _countof(vertexInputLayoutInfo),\n    vertexShaderBlob-&gt;GetBufferPointer(),\n    vertexShaderBlob-&gt;GetBufferSize(),\n    &amp;_vertexLayout)))\n{\n    std::cout &lt;&lt; \"D3D11: Failed to create default vertex input layout\\n\";\n    return false;\n}\n</code></pre> <p>As usual, we follow the same pattern, we pass in our <code>vertexInputLayoutInfo</code> that we  just created and its size, we also need to pass our vertex blob pointer and size, and  finally our output parameter which is our input layout.</p> <p>Now all we have to do is create a vertex buffer (do not worry it's really easy) and  issue our first <code>Draw</code> command!</p> <p>Error</p> <p>Image showing stride and offset?</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#vertex-buffers","title":"Vertex Buffers","text":"<p>Vertex Buffers might seem hard at first, but they're really nothing more than a  buffer that resides in our device's memory, which means really fast access. This  buffer will be then bound and read by the vertex shader.</p> <p>Creating a vertex buffer is also really easy, first we have to make some data to put  in our buffer, since we want to draw a triangle, we will be creating 3 vertices using  our <code>VertexPositionColor</code> struct.</p> <pre><code>constexpr VertexPositionColor vertices[] =\n{\n    { Position{  0.0f,  0.5f, 0.0f }, Color{ 0.25f, 0.39f, 0.19f } },\n    { Position{  0.5f, -0.5f, 0.0f }, Color{ 0.44f, 0.75f, 0.35f } },\n    { Position{ -0.5f, -0.5f, 0.0f }, Color{ 0.38f, 0.55f, 0.20f } },\n};\n</code></pre> <p>Remember, the position coordinates we have to give to the vertex shader  must be in range [-1.0, 1.0] by the time the vertex shader  stage ends, otherwise we will not be able to see that  vertex - because of this we supply vertices as [-1.0, 1.0]. </p> <p>We are storing coordinates that form our triangle  here: a Position and Color component per vertex</p> <p>If you want you can try to visualize the triangle, take a piece of paper, draw a  Cartesian Plane, draw 3 points and connect the dots with these coordinates.</p> <p>Now that we have the data, let's store this on our vertex buffer:</p> <pre><code>D3D11_BUFFER_DESC bufferInfo = {};\nbufferInfo.ByteWidth = sizeof(vertices);\nbufferInfo.Usage = D3D11_USAGE::D3D11_USAGE_IMMUTABLE;\nbufferInfo.BindFlags = D3D11_BIND_FLAG::D3D11_BIND_VERTEX_BUFFER;\n\nD3D11_SUBRESOURCE_DATA resourceData = {};\nresourceData.pSysMem = vertices;\nif (FAILED(_device-&gt;CreateBuffer(\n    &amp;bufferInfo,\n    &amp;resourceData,\n    &amp;_triangleVertices)))\n{\n    std::cout &lt;&lt; \"D3D11: Failed to create triangle vertex buffer\\n\";\n    return false;\n}\n</code></pre> <p>We begin by filling a <code>bufferInfo</code> descriptor for our buffer, we specify how many  bytes we want, since this buffer will never change, for the <code>Usage</code> we specify:  <code>D3D11_USAGE_IMMUTABLE</code> (the buffer is unmodifiable), this  lets <code>D3D11</code> put this data as close as possible to the  GPU, finally we specify how we want to use this buffer, we want this to be a vertex  buffer, so for the <code>BindFlags</code> we give: <code>D3D11_BIND_VERTEX_BUFFER</code>.</p> <p>And finally we create a <code>resourceData</code>, and populate the only field we care about:  <code>pSysMem</code>, which is a pointer to our vertices which are currently in system RAM.</p> <p>Then we issue the creation of the buffer using <code>CreateBuffer</code>, using the information  we collected until now.</p>"},{"location":"1-introduction/1-1-getting-started/1-1-3-hello-triangle/#drawing-our-triangle","title":"Drawing our triangle","text":"<p>Now, we have reached the moment of truth, in our <code>Render</code> function we will add a few  things:</p> <pre><code>_deviceContext-&gt;IASetInputLayout(_vertexLayout.Get());\n</code></pre> <p>This sets the input layout we want to use.</p> <pre><code>_deviceContext-&gt;IASetVertexBuffers(\n    0,\n    1,\n    _triangleVertices.GetAddressOf(),\n    &amp;vertexStride,\n    &amp;vertexOffset);\n</code></pre> <p>Then we go ahead and bind our vertex buffer.</p> <pre><code>_deviceContext-&gt;IASetPrimitiveTopology(D3D11_PRIMITIVE_TOPOLOGY::D3D11_PRIMITIVE_TOPOLOGY_TRIANGLELIST);\n</code></pre> <p>This sets how the Input Assembler should interpret the vertex data, since we want to  draw triangles, <code>D3D11_PRIMITIVE_TOPOLOGY_TRIANGLELIST</code> is the right flag for us.</p> <pre><code>_deviceContext-&gt;VSSetShader(\n    _vertexShader.Get(),\n    nullptr,\n    0);\n_deviceContext-&gt;PSSetShader(\n    _pixelShader.Get(),\n    nullptr,\n    0);\n</code></pre> <p>Setting the vertex and pixel shader here. I suggest to put <code>PSSetShader</code> after  <code>RSSetViewports</code>, since it will maintain the pipeline order, it doesn't have any  performance or correctness implications, it will just help you remember better which  stage comes after which.</p> <pre><code>_deviceContext-&gt;Draw(3, 0);\n</code></pre> <p>And finally, tell the GPU to draw 3 vertices, this will invoke the vertex shader 3  times, and it will successfully process the 3 vertices we put in our vertex buffer.</p> <p>Finally, let's review all the commands we issue in <code>Render</code></p> <pre><code>_deviceContext-&gt;IASetInputLayout(_vertexLayout.Get());\n_deviceContext-&gt;IASetVertexBuffers(\n    0,\n    1,\n    _triangleVertices.GetAddressOf(),\n    &amp;vertexStride,\n    &amp;vertexOffset);\n_deviceContext-&gt;IASetPrimitiveTopology(D3D11_PRIMITIVE_TOPOLOGY::D3D11_PRIMITIVE_TOPOLOGY_TRIANGLELIST);\n_deviceContext-&gt;VSSetShader(\n    _vertexShader.Get(),\n    nullptr,\n    0);\n_deviceContext-&gt;RSSetViewports(\n    1,\n    &amp;viewport);\n_deviceContext-&gt;PSSetShader(\n    _pixelShader.Get(),\n    nullptr,\n    0);\n_deviceContext-&gt;OMSetRenderTargets(\n    1,\n    _renderTarget.GetAddressOf(),\n    nullptr);\n</code></pre> <p>As you can see, we go through the pipeline in an orderly fashion, and although we  do not use all the stages, we can see the top-to-bottom execution of the stages, IA </p> <p>(Input Assembler) -&gt; VS (Vertex Shader) -&gt; RS (Rasterizer Stage) -&gt; PS (Pixel Shader)  -&gt; OM (Output Merger).</p> <p>You should now be able to run this and see your first triangle!</p> <p>Error</p> <p>Provide picture of window with triangle</p> <p>Project on GitHub</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-0-overview/","title":"Overview","text":"<p>While developing your graphics engine, your game, or your small tool to visualize stuff, you sometimes end up in situations where you don't know what is going on, or can't explain why this dang triangle is not showing up, while the screen is black.</p> <p>There are various tools and techniques which can help us getting the triangle on the screen.</p> <p>Clear State</p> <p>Debug Layer</p> <p>Naming Things</p> <p>RenderDoc</p> <p>Laptop GPUs</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-1-clear-state/","title":"Clear State","text":"<p>The Clear State is useful to make sure no state from previous drawcalls is still being used.</p> <p>For example:</p> <p>Say we have just drawn our scene, all models and such are being drawn properly, but now we decide to add some UI, we adjust some state that normally is set-once and forget (at initialisation) like scissor rects or viewports. But now after we're done adding our UI element draw, suddenly our models are drawing in some weird square on the screen, or models that perhaps did not have a normal-map suddenly use a UI texture as if it were one.</p> <p>This is something that can happen if we don't have a clear state, and really only goes wrong if one did not think they'd had to set some part of the pipeline during some draws.</p> <p>The usefulness of a Clear State generally comes in being able to enable or disable it at will and it being used as a mechanic to spot rendering bugs caused by state that was not (re)set correctly. So as soon as weird things start to happen, you can use a clear state between say every draw, and if the bug then disappears you'll know it's very likely related to state set from a previous draw.</p> <p>One can accomplish this in a few ways:</p> <p>Setting their states/buffers/resources/targets to NULL where possible by hand.</p> <p>Or call a more \"reset it all\" function that exists on the ID3D11DeviceContext aptly called ClearState().</p> <p>Info</p> <p>https://docs.microsoft.com/en-us/windows/win32/api/d3d11/nf-d3d11-id3d11devicecontext-clearstate</p> <p>Though keep in mind that using the latter function also resets stuff like the inputlayout, primitive topology and literally everything. This might require a bit more work in making sure all rendering state is setup correctly again afterwards</p> <p>In pure performance terms it can be quite wasteful to reset a whole bunch of state (or re-set it) every draw/pass/frame which is why this is viewed as a debugging option.</p> <p>In the end, one generally should make sure their draws always set (or have set) their required state so they do not need to use a clear-state.</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-2-debug-layer/","title":"Debug Layer","text":"<p>During development it is important to have tools to help you. Be it pointing out typos, obvious error or simple hints why things might not draw on screen.</p> <p>D3D11 provides a debug layer, which can give you hints, warnings and errors when you put in the wrong values into calls for d3d11 functions.</p> <p>This debug layer needs to be actively enabled, which is a simple flag you have to set when creating the device.</p> <p>Right now we have thee <code>deviceFlags</code></p> <pre><code>UINT deviceFlags = D3D11_CREATE_DEVICE_FLAG::D3D11_CREATE_DEVICE_BGRA_SUPPORT;\n</code></pre> <p>and change it to</p> <pre><code>    UINT deviceFlags = D3D11_CREATE_DEVICE_FLAG::D3D11_CREATE_DEVICE_BGRA_SUPPORT;\n#if !defined(NDEBUG)\n    deviceFlags |= D3D11_CREATE_DEVICE_FLAG::D3D11_CREATE_DEVICE_DEBUG;\n#endif\n</code></pre> <p>Obviously this makes sense when you are running this in the <code>Debug</code> configuration where the preprocessor variable <code>DEBUG</code>/<code>_DEBUG</code> is defined, or rather where when is <code>NDEBUG</code> is not defined :) (like it is in <code>Release</code> configuration).</p> <p>in <code>DebugLayer.hpp</code> we have to add the following member variable</p> <pre><code>ComPtr&lt;ID3D11Debug&gt; _debug = nullptr;\n</code></pre> <p>and we initialize it in <code>DebugLayer.cpp</code>'s <code>Initialize</code>, right after we create device and device context.</p> <pre><code>if (FAILED(_device.As(&amp;_debug)))\n{\n    std::cout &lt;&lt; \"D3D11: Failed to get the debug layer from the device\\n\";\n    return false;\n}\n</code></pre> <p>and we also adjust the destructor too</p> <pre><code>    _deviceContext.Reset();\n#if !defined(NDEBUG)\n    _debug-&gt;ReportLiveDeviceObjects(D3D11_RLDO_FLAGS::D3D11_RLDO_DETAIL);\n    _debug.Reset();\n#endif\n    _device.Reset();\n</code></pre> <p><code>ReportLiveDeviceObjects</code> even tells us at the end of the application which and how many objects are still alive from our d3d11 adventure.</p> <p>It will help us track leaks in the future.</p> <p>Now try changing the following value to see the actual debug layer in action.</p> <p>Find</p> <pre><code>swapChainDescriptor.BufferCount = 2;\n</code></pre> <p>and change it to</p> <pre><code>swapChainDescriptor.BufferCount = 1;\n</code></pre> <p>The debug layer will yell at you, in the Console window of Visual Studio:</p> <pre><code>DXGI ERROR: IDXGIFactory::CreateSwapChain: Flip model swapchains (DXGI_SWAP_EFFECT_FLIP_SEQUENTIAL and DXGI_SWAP_EFFECT_FLIP_DISCARD)\nrequire BufferCount to be between 2 and DXGI_MAX_SWAP_CHAIN_BUFFERS, inclusively. DXGI_SWAP_CHAIN_DESC{ SwapChainType = ..._HWND,\nBufferDesc = DXGI_MODE_DESC1{Width = 3456, Height = 1944, RefreshRate = DXGI_RATIONAL{ Numerator = 0, Denominator = 0 },\nFormat = B8G8R8A8_UNORM, ScanlineOrde\n</code></pre> <p>or find</p> <pre><code>_deviceContext-&gt;Draw(3, 0);\n</code></pre> <p>and change it to</p> <pre><code>_deviceContext-&gt;Draw(6, 0);\n</code></pre> <p>and the debug layer will tell you thats not cool (you cant draw more triangles than there are defined in the vertexbuffer/bound to the input assembly)</p> <pre><code>D3D11 WARNING: ID3D11DeviceContext::Draw: Vertex Buffer at the input vertex slot 0 is not big enough for what\nthe Draw*() call expects to traverse. This is OK, as reading off the end of the Buffer is defined to return 0.\nHowever the developer probably did not intend to make use of this behavior.  [ EXECUTION WARNING #356: DEVICE_DRAW_VERTEX_BUFFER_TOO_SMALL]\n</code></pre> <p>Explain InfoQueue</p> <p>How to mute certain messages</p> <p>Explain InfoQueue via dxcpl</p> <p>How to do that via the control panel</p> <p>One thing which should go without saying, the debug layer will slow down your application a bit.</p> <p>Project on GitHub</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-3-naming-things/","title":"Naming things","text":"<p>The debug layer might tell us possible leaks of various d3d11 related objects, the cool thing is, it can tell you exactly which object is leaking, via its name.</p> <p>We can give our d3d11 objects a name.</p> <p>Each D3D11 object (not the device itself) derives from <code>ID3D11DeviceChild</code> and that interface implements a method <code>SetPrivateData</code> which we can use to assign a name to all those objects.</p> <p>For that we will introduce a macro (hopefully the only one :))</p> <pre><code>template&lt;UINT TDebugNameLength&gt;\ninline void SetDebugName(\n    _In_ ID3D11DeviceChild* deviceResource,\n    _In_z_ const char(&amp;debugName)[TDebugNameLength])\n{\n    deviceResource-&gt;SetPrivateData(WKPDID_D3DDebugObjectName, TDebugNameLength - 1, debugName);\n}\n</code></pre> <p>And we use it like</p> <pre><code>SetDebugName(_deviceContext.Get(), \"CTX_Main\");\n</code></pre> <p>Unfortunately not every object is/implements <code>ID3D11DeviceChild</code> so for the other things like the dxgi factory or the device itself we have to use <code>SetPrivateData</code> the ordinary way</p> <pre><code>constexpr char factoryName[] = \"Factory1\";\n_factory-&gt;SetPrivateData(WKPDID_D3DDebugObjectName, sizeof(factoryName), factoryName);\n\nconstexpr char deviceName[] = \"DEV_Main\";\n_device-&gt;SetPrivateData(WKPDID_D3DDebugObjectName, sizeof(deviceName), deviceName);\n</code></pre> <p>Now run the thing again and take a look at the summary report of the debug device in the output window</p> <pre><code>D3D11 WARNING: Live ID3D11Device at 0x000002214E8A1B10, Name: DEV_Main, Refcount: 3 [ STATE_CREATION WARNING #441: LIVE_DEVICE]\nD3D11 WARNING:  Live ID3D11Context at 0x000002214E8A5B50, Name: CTX_Main, Refcount: 0, IntRef: 1 [ STATE_CREATION WARNING #2097226: LIVE_CONTEXT]\n</code></pre> <p>Notice anything?</p> <p>Exactly, they show names.</p> <p>Naming things is also useful while debugging your application. In debuggers such as RenderDoc or Nvidia Nsight, the resource name will show up in the Resource Inspector, making it easier to find your resources and check its contents / configuration.</p> <p>For example, if we name our Triangle vertex buffer using the follows <pre><code>SetDebugName(_triangleVertices.Get(), \"Triangle_Vertices\");\n</code></pre> In RenderDoc's Resource Inspector tab, we see </p> <p>Similary, in Nsight's All Resources tab, we see </p> <p>Project on GitHub</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-4-renderdoc/","title":"RenderDoc","text":"<p>Typically while developing an application there will be complications that cannot be easily deduced by watching the program execute or by reading the code; in a situation like this one will use a debugger to monitor the state of the program to locate the bug, and this is no different for graphics programming.</p> <p>Graphics programmers will use what is called a \"graphics debugger\" when working with a graphics API to monitor API calls, pipeline state, data within the pipeline, etc.</p> <p>While there are multiple graphics debuggers provided by multiple vendors such as Nvidia's Nsight, AMD's Radeon Graphics Profiler (Does not directly support DX11), Intel's Graphics Performance Analyzers, and Microsoft's PIX, we will instead be using a cross-platform open-source option, RenderDoc.</p> <p>To download RenderDoc, click here and follow the instructions on the webpage.</p> <p></p> <p>Before getting into the details of Renderdoc, an executable is needed to display what the application is capable of.</p> <p></p> <p>In the \u201cExecutable Path\u201d navigate to the directory of the last section\u2019s project and set it as the executable. Set the \u201cWorking Directory\u201d to the directory where the asset and shader folders are kept.</p> <p>RenderDoc can now be started by clicking the \u201cLaunch\u201d button in the bottom right of the \u201cLaunch Application\" tab. Upon launching the application it will have an overlay in the top left corner stating the active API, frame count, frame time, captures saved, and to press F12 to take a frame capture.</p> <p></p> <p>After a frame has been captured and the captured application is closed, return to RenderDoc.</p> <p></p> <p>The RenderDoc window is now fully populated, however for the sake of brevity only the \u201cEvent Browser\u201d, \u201cAPI Inspector\u201d, \u201cPipeline State\u201d, and \u201cResource Inspector\u201d are of concern.</p> <p></p> <p>The event browser displays a generalization of the API calls called in the form of events, an event is one or more API calls that have been grouped together by relation.</p> <p>There is not much going on in this example, but it can be helpful when debugging scenes making hundreds or even thousands of draws and dispatches a frame.</p> <p></p> <p>The API Inspector displays the contents of an event by listing out the API calls that it contains. For example, all the calls made to set the state for the draw call are grouped together under an event called \u201cDraw\u201d.</p> <p></p> <p>The Pipeline State tab displays the state of each active pipeline stage in addition to bound resources for the currently selected event.</p> <p>In this instance, the hull, domain, and geometry shader stages are grayed out because they were not used for that draw.</p> <p>The compute shader stage is called with its own dispatch commands and therefore is never active with the other stages; it is not a part of the rasterization pipeline.</p> <p></p> <p>The Resource Inspector presents a list of all the resources used to render the frame on the right side of the window with information pertaining to its usage within the frame, related resources, and the functions used to initialize the resource.</p> <p>To learn more about RenderDoc, click here for its documentation.</p> <p>Next chapter</p>"},{"location":"1-introduction/1-2-debug/1-2-5-laptop-gpus/","title":"Laptop GPUs","text":"<p>Some laptops come with dedicated graphics cards not just integrated ones.</p> <p>Sometimes default settings let your application only use the integrated gpu, which usually is not the best to run anything but very light games. Some games and programs have a way to select the desired graphics card, you can chose which graphics card to use via the control panel of your driver.</p> <p>You can also enforce the use of the dedicated graphics card per code. Thats what we are going to show here.</p> <p>Simply find a spot in your program and add the following code</p> <pre><code>extern \"C\"\n{\n   __declspec(dllexport) DWORD NvOptimusEnablement = 0x00000001;\n\u200b   __declspec(dllexport) int AmdPowerXpressRequestHighPerformance = 1;\n}\n</code></pre> <p>This will enforce graphics cards from NVIDIA and AMD to use the dedicated graphics card, when available.</p> <p>You can find more information here for NVIDIA and here for AMD</p> <p>Intel might have something similar, once they release their dedicated graphics cards for laptops.</p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-0-overview/","title":"Overview","text":"<p>Now that we know how to get a triangle on screen, we'll dive into the more simple topics in rendering.</p> <p>We'll talk about the rasterizer state and what it can do for us, get some textures onto our geometry, and explain everything we need to get a 3D model properly rendering.</p> <p>Finally we'll add a tool that almost everyone uses, \"Dear ImGui\" which allows is to very flexibly add some UI to control things in our scenes.</p> <p>Rasterizer State</p> <p>Texturing</p> <p>Setting up 3D Rendering</p> <p>3D Rendering</p> <p>Models</p> <p>Dear ImGui</p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-1-rasterizer-state/","title":"Rasterizer State","text":"<p>The rasterizer state, as the name implies, controls the rasterizer.</p> <p>This is a key component in rendering, despite it looking rather trivial, rendering as we normally do (advanced techniques aside) is called \"Rasterized Rendering\", this is how our geometry gets plotted to our pixels on screen, how a triangle is handled and filled and how data gets passed to other shader stages.</p> <p>Let's step into the few variables we can control in D3D11.</p> <p>The first one is the FillMode, this only has two options and simply tells the rasterizer whether to completely fill a triangle, or to only show its edges (or as is more commonly known as \"to display a wireframe\").  That's right, we don't need a fancy shader or render the geometry in lines in order to display a wireframe, it's a built-in feature of the rasterizer, one mostly used for debugging or special kinds of visualisation.</p> <p></p> <p>The CullMode is a bit more useful, this controls when we cull certain triangles, and has three options, <code>Front</code>, <code>Back</code> and <code>None</code>. For most purposes we generally set this to <code>Back</code>, so what does this actually do?</p> <p>Depending on our vertex winding order (which we can control in the next discussed flag), it knows what side of the triangle is the \"front side\", the rasterizer can automatically ignore triangles depending on whether they are facing us or not (or just simply render everything). As was said before, generally we're not concerned with triangles that are facing away from us, as we normally can't see them anyways (you can't see the back-side of a sphere, so why render it?). However it does have its uses to do otherwise.</p> <p>Think of foliage or tree leaves for example, it's way more helpful to only place a single plane of geometry and render it from both sides than to duplicate all the geometry resulting in the GPU having to do a lot more work.</p> <p>Some shadowing techniques may also rely on Frontface culling in order to get better results, but we won't go into detail about that here now. </p> <p>Simple to explain, but perhaps a bit harder to understands is the \"Vertex Winding Order\" which is controlled by our input geometry as well as the rasteriser state.</p> <p>Shortly explained, if <code>FrontCounterClockwise</code> is <code>true</code>, then a triangle is front facing if the vertices are in a counter-clockwise order , otherwise it is back facing.</p> <p>The following vertex order makes up a \"clockwise\" triangle, so if the <code>FrontCounterClockwise</code> is true, this means this triangle is facing away from us, and if <code>CullMode</code> is <code>Back</code> this means we skip this entire triangle. <pre><code>     0\n    /\\\n   /  \\\n  /    \\\n /______\\ \n2        1 \n</code></pre> For completeness-sake, the following is a \"counter-clockwise triangle\". <pre><code>     0\n    /\\\n   /  \\\n  /    \\\n /______\\ \n1        2 \n</code></pre></p> <p>How it looks to have our triangle in <code>Front</code> and <code>Back</code> cull-modes. </p> <p>(Yes, the \"missing\" triangle is the expected result here, it's being culled after all!)</p> <p>The last few variables we can control require some knowledge on topics we'll cover in later chapters, for now the only important one that is set to <code>true</code> by default is: <code>DepthClipEnable</code>, which allows the rasterizer to discard triangles, or more correctly \"fragments\" that fall beyond our depth-range from the viewport.</p> <p>We'll go into more detail for the rest once it becomes relevant, as well as the various depth-related variables.</p> <p>If curious none-the-less, feel free to read up on it with the official documentation at: D3D11_RASTERIZER_DESC</p> <p>Next chapter </p>"},{"location":"1-introduction/1-3-basics/1-3-2-texturing/","title":"Texturing","text":"<p>Project on GitHub</p> <p>Now that we have a triangle, the next step is getting a texture onto that triangle. </p> <p>However in order to get there we need to take a few steps.</p> <p>Textures are usually created from images, however these images almost always come in some compressed form. We cannot just simply read in a PNG file and create a texture from it, the GPU does not understand the PNG format (or most other formats) with the exception of a few special formats.</p> <p>Thus in order to create a texture from an image we'll have to decode it, this can be a very difficult process so we will use a library for it, in our case two even! We'll quickly explain both <code>FreeImage</code> and <code>DirectXTex</code> here but you can use whatever you are comfortable with in the end, we have chosen these two libraries because they both do specific things.</p> <ul> <li><code>FreeImage</code> will allow us to read nearly every common format (PNG,JPG,TGA,BMP, etc) and give us a few options to preprocess them.</li> <li><code>DirectXTex</code> will allow us to read in the DDS format, which will be explained in a bit.</li> </ul> <p>Let's take a look at the code, we'll mainly focus on two functions here, <code>CreateTextureView</code> and <code>CreateTextureViewFromDDS</code>.</p> <p>Firstly, <code>CreateTextureView</code>: In this function we'll use <code>FreeImage</code> to read in regular images and create a texture from it. After reading in the full file <code>CreateFileW()</code> -&gt; <code>ReadFile()</code> we tell <code>FreeImage</code> to open a handle to the raw image data (which is still encoded) with: <code>FreeImage_OpenMemory(fileDataRaw.data(), fileDataRaw.size())</code>. We'll query the image format with <code>FreeImage_GetFileTypeFromMemory(memHandle)</code>, which will attempt to figure out the file type from the image's \"magic\" bytes (most files start with a few bytes that will tell applications what kind of file it is, for example PNGs start with: <code>89 50 4E 47</code> which in ascii is: <code>\u2030PNG</code>).</p> <p>If <code>FreeImage</code> wasn't able to figure out the format, we'll return a <code>nullptr</code> because we won't be able to decode the image (if it was one in the first place), otherwise we'll start the decoding process with <code>FreeImage_LoadFromMemory(imageFormat, memHandle)</code> which will decode the image to a raw format consisting out of (usually) RGB(A) colors.  After that is done we can get rid of the raw memory as we no longer need it (we have the decoded data now). Something that isn't entirely required to do, but we will do for consistency is call <code>FreeImage_FlipVertical(image)</code>, D3D11's UV.y space are usually going up, where most other software expects it to go down, so naively loading images can result in textures being displayed upside down.  <code>DirectXTex</code> will handle this for us, so for consistency we'll flip the image as well.</p> <p>Now that we have the image in (almost) the format we need, we're getting to the texture creation. The sole exception being if the original texture was 24-bit (RGB), as there exist no 24-bit (R8G8B8) texture formats in D3D11 we'll need to convert it to a 32-bit (R8G8B8A8) texture before doing anything, this is easily done with <code>FreeImage_ConvertTo32Bits(image)</code>.</p> <p>Now we get to the D3D11 stuff. In order to create a texture we need a few things, its width, height and format. This will tell D3D11 what data we're giving it and how it should interpret it.  This data will go in a <code>D3D11_TEXTURE2D_DESC</code> which \"DESC(ribes)\" our texture data.</p> <p>Most images we see in our daily lives will be 24-bit or 32-bit, depending if they have alpha or not. As was noted before, D3D11 does not support 24-bit textures, but what do these values mean? The amount of \"Bits per Pixel\" is what we're talking about when we say an image is N-bit, this commonly translates to \"how many channels\" a texture has, in order to not overcomplicate things we'll assume all data we're working on is SDR (Standard Dynamic Range) and not HDR (High Dynamic Range) The key difference between those two is the range of a color, in SDR colors usually have 8 bits per channel, meaning our colors go from 0 to 255 (0xFF), HDR can take many shapes or forms and has no real consistent format to speak of but values can easily surpass the range of 0 to 1024 or more.</p> <p>Knowing that we assume SDR formats we'll <code>switch</code> on the amount of BPP of common values, anything weird and we're out! This is the first spot where we get into texture formats, our first case is \"8 bits per pixel\", not very common, but easy to handle, for us this means there only exists an \"R\" channel, no \"GBA\" to speak of. We'll tell D3D11 the same by specifying <code>DXGI_FORMAT_R8_UNORM</code>.</p> <p>You'll see the enum ends on <code>_UNORM</code>, this is 'how' our texture sampler will return the data to us in the shader. The following data types exist:</p> Suffix Explanation <code>UNORM</code> \"Unsigned Normalized\", data will be interpreted as a range of 0.0f to 1.0f. <code>SNORM</code> \"Signed Normalized\", data will be interpreted as a range of -1.0f to 1.0f. <code>UINT</code> \"Unsigned Integer\", data will be interpreted as a maximum range of 0 to 4294967295 (0xFFFFFFFF). <code>SINT</code> \"Signed Integer\", data will be interpreted as a maximum range of -2147483648 to 2147483647. <code>FLOAT</code> Each channel will be a full floating point value. <code>TYPELESS</code> This format is special and can be many things, its up to the shader how to handle this data. <p>There exists another suffix behind these ones for some special cases: <code>SRGB</code> (<code>DXGI_FORMAT_R8G8B8A8_UNORM_SRGB</code>) which will tell the sampler that the data is in sRGB color space.</p> <p>As we'll assume normal SDR textures with 8 bits per pixel, we end up with either <code>R8</code>, <code>R8G8</code>, or <code>R8G8B8A8</code> depending on how many BPP the image has. We'll also use <code>UNORM</code> as the math in shaders becomes a lot easier if everything is a neat 0.0f to 1.0f range.</p> <p>To keep things simple we set ArraySize, MipLevels and SampleDesc.Count to 1, and these will be explained in later chapters. After also filling in the Width, Height and Format we're left with two more members <code>Usage</code> and <code>BindFlags</code>.</p> <p><code>Usage</code> will tell D3D11 what we're going to do with the texture data, will we adjust it or is it set in stone from the start?  This is what we tell it with:</p> Flag Explanation <code>D3D11_USAGE_DEFAULT</code> \"We can read and write to this on the GPU\" (For textures we want to change on the GPU later, usually through shaders). <code>D3D11_USAGE_IMMUTABLE</code> \"Set in stone, not adjustable at all once we create the texture\". (For textures that 'never' change). <code>D3D11_USAGE_DYNAMIC</code> \"The GPU can only read this, and the CPU can only write\". (For textures that we infrequently change through the CPU). <code>D3D11_USAGE_STAGING</code> \"This texture is copied to/from often (either on CPU or GPU)\" (This 'only' supports copying, not arbitary writing). <p><code>BindFlags</code> will tell D3D11 how we're going to use the texture, there are multiple values that do not make sense (or are valid) for textures, so we'll only explain the valid ones:</p> Flag Explanation <code>D3D11_BIND_SHADER_RESOURCE</code> This a read-only resource for shaders (This is the one we use for regular textures!) <code>D3D11_BIND_STREAM_OUTPUT</code> This is for our swapchain output, very rarely used. <code>D3D11_BIND_RENDER_TARGET</code> This a write-only resource (render target) <code>D3D11_BIND_DEPTH_STENCIL</code> This is a special (write-only) DepthStencil resource (use for depth buffers) <code>D3D11_BIND_UNORDERED_ACCESS</code> This is a read-write resource for shaders. <p>Because we don't have plans to change the texture after creation and we only use it as a read-only resource for shaders, we set Usage to <code>D3D11_USAGE_IMMUTABLE</code> and BindFlags to <code>D3D11_BIND_SHADER_RESOURCE</code> However because it is IMMUTABLE we need to supply the data immediately upon creation, we'll do this with a <code>D3D11_SUBRESOURCE_DATA</code> parameter. This is a simple structure that tells D3D11 where to find the data and how big it is. For <code>pSysMem</code> we point it to the data that <code>FreeImage</code> is holding which we can get with: <code>FreeImage_GetBits(image)</code>. <code>SysMemPitch</code> will tell D3D11 how big a row of data is, as we have only told it how to interpret the data in <code>D3D11_TEXTURE2D_DESC</code> but not what it 'actually' is. The size of a row of data from a 32-bit texture with a width of 512 is calculated as following:  We can get the amount of channels (we're assuming 8 bits each) by dividing the image's BPP (Bits Per Pixel) by 8 (so our resulting channel count is 4), we multiply this with the width, and so we end up with our example being: <code>(32/8) * 512 = 2048</code> bytes per row.</p> <p>And that's it.. now we can finally create our texture resource by calling: <code>device-&gt;CreateTexture2D(&amp;textureDesc, &amp;initialData, texture.GetAddressOf())</code>. Alas we're not there yet, we have created the resource but have not created our view which is what we need to bind to a shader stage. We do this by creating a ShaderResourceView, thankfully all the hard work is already done and we can simply fill in the few bits in a <code>D3D11_SHADER_RESOURCE_VIEW_DESC</code> by copying most of it from our <code>D3D11_TEXTURE2D_DESC</code>, the only thing new here is us telling it that the view should interpret the data as a 2D texture by specifying <code>D3D11_SRV_DIMENSION_TEXTURE2D</code>.</p> <p>We call <code>device-&gt;CreateShaderResourceView(texture.Get(), &amp;srvDesc, &amp;srv))</code> and end up with our most important piece, an <code>ID3D11ShaderResourceView</code>, at this point we no longer care about the original resource data (<code>WRL::ComPtr&lt;ID3D11Texture2D&gt; texture</code>) and let it go out of scope, which will decrease the reference count for us. This works because the ShaderResourceView holds an internal reference to the Texture2D so the actual data is not deleted until we get rid of our <code>ID3D11ShaderResourceView</code>.</p> <p>Before we do anything with it (other than storing it) we'll quickly go over <code>CreateTextureViewFromDDS()</code>. We have made DDS a special case because its not a \"normal image\", its a rather special and efficient image format because its compressed in a way that the GPU understands allowing us to save memory and bandwidth. Because we have gone through the path of manually creating a texture so we know what happens under the hood a bit better, we can now allow ourselves to have all of this taken care of for us.</p> <p>Getting DDS in a CPU/Human readable format is a big hassle, and luckily we don't even want to, it pretty much goes to the GPU as is and it already contains the format and everything inside of it. All we need to do here is call <code>DirectX::LoadFromDDSFile()</code> and proceedingly <code>DirectX::CreateTexture()</code> in order to get our texture resource (<code>WRL::ComPtr&lt;ID3D11Resource&gt;</code>), at this point we could make our own ShaderResourceView if we want specific formats or also let <code>DirectXTex</code> take of it (which we will). We choose the latter and call <code>DirectX::CreateShaderResourceView</code> and we end up with a <code>ID3D11ShaderResourceView</code> ready to use!</p> <p>Now that we have our SRV ((S)hader(R)esource(V)iew) ready, we need to do a few more things, bind it to the shader stage, add it to the shader code and create and add a texture sampler. Thankfully these steps aren't overly complicated.</p> <p>Binding it to a shader stage (in our case the pixel shader), is as simple as getting the ID3D11DeviceContext and call <code>PSSetShaderResources</code> on it. (XX)SetShaderResources (where XX is \"VS\" (VertexShader), \"PS\" (PixelShader),  etc) takes a few parameters, as it can set multiple textures at once:</p> Parameter Explanation <code>StartSlot</code> The texture slot to start from (generally starting at 0). <code>NumViews</code> The amount of textures we're setting. <code>ppShaderResourceViews</code> The array of SRVs. <p>If we only need to set a single texture, this is as simple as calling: <code>PSSetShaderResources(0, 1, srv.Get())</code>. If we call <code>PSSetShaderResources(2, 2, srvs)</code> it will start at slot 2 and thus set slot 2 and 3, with the SRV's supplied.</p> <p>Creating a SamplerState (which is used for sampling textures) is pretty simple, all it needs is us telling how to filter the texture and how it wraps (or not) around the texture if we exceed the bounds. Filling in a <code>D3D11_SAMPLER_DESC</code> only requires a few members to be set: \"Filter\", \"AddressU\", \"AddressV\", \"AddressW\". Filter takes a <code>D3D11_FILTER</code>, which has a lot of options but all compose out of a few core concepts:</p> Prefix Explanation <code>MIN_MAG</code> The filter used across the texture's pixels regardless of angle/distance. <code>MIP</code> The filter used between the texture's mips. (we'll come back to this later) <p>And the filter modes:</p> Suffix Explanation <code>POINT</code> \"no filtering\" pixels are shown as is (good for sprite/pixel art!). <code>LINEAR</code> pixels are smoothed between each other, cheap pick for 3D games. <code>ANISOTROPIC</code> mode for doing additional processing for textures at angles, looks like LINEAR but cleaner when the texture is seen at big angles (bit more expensive than LINEAR, but great pick for anything 3D). <p>In our example we pick <code>D3D11_FILTER_MIN_MAG_LINEAR_MIP_POINT</code>, but feel free to experiment with these.</p> <p>AddressU/V/W all take the same enum, where U is behaviour on the horizontal axis, V is vertical, and W is depth (for 3D textures). The following options exist and all work on behaviour when sampling outside the 0.0f to 1.0f range:</p> Flag Explanation <code>D3D11_TEXTURE_ADDRESS_WRAP</code> Wrap around (akin to modulo(1.0f)) (good for tiling textures!) <code>D3D11_TEXTURE_ADDRESS_MIRROR</code> Flips the texture every time you hit the border. <code>D3D11_TEXTURE_ADDRESS_CLAMP</code> Samples the pixel at the edge <code>D3D11_TEXTURE_ADDRESS_BORDER</code> Return the border value specified in the SamplerState <code>D3D11_TEXTURE_ADDRESS_MIRROR_ONCE</code> Mirrors the texture once, then clamps <p>A few examples: </p> <p>We create our SamplerState with <code>_device-&gt;CreateSamplerState(&amp;linearSamplerStateDescriptor, &amp;_linearSamplerState)))</code> and binding it works the same as the SRVs but with (XX)SetSamplers instead.  For example: <code>deviceContext-&gt;PSSetSamplers(0, 1, sampler.GetAddressOf())</code>.</p> <p>A visualisation of what these \"slots\" are can be found within Renderdoc:</p> <p></p> <p>As an example you can see in the image above that there is a texture bound at slot 0, and a sampler at slot 0.</p> <p>Lastly we need to tell the pixel shader that we can now use a texture in there. We do this by adding:</p> <pre><code>sampler LinearSampler : register(s0);\nTexture2D Texture : register(t0);\n</code></pre> <p>This is where we first encounter the <code>register</code> keyword, depending on what we give it, this needs to match the slot we set the resource in (as specified above). Because we set both the texture and sampler to slot 0, these end up being <code>t0</code> and <code>s0</code> (<code>t</code> for texture, and <code>s</code> for sampler). We also created a 2D texture so our type for the texture is <code>Texture2D</code>.</p> <p>Finally, we can sample the texture by calling the function <code>Sample</code> on <code>Texture</code>, and for the first parameter we supply our sampler, and the second argument will be our location (in UV space) <pre><code>float4 texel = Texture.Sample(LinearSampler, input.Uv);\n</code></pre> This will give us the color of the pixel at the specified location, in our case, blended with the neighbouring pixels because we specified a linear filter.</p> <p>We end up with a froge on the triangle! </p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-3-setting-up-3d-rendering/","title":"Setting up 3D Rendering","text":"<p>Now that we know how to work with simple predefined 2D geometry, it's time to take the next step.</p> <p>Here we dive into the 3rd dimension, but before we do that, we need to explain some things we're going to need in order to properly display a 3D object.</p> <p>In this chapter we'll talk about the following items:</p> <ul> <li>Math ( The matrix compositions we need to get from 3D to our screen (2D) )</li> <li>Constant Buffers</li> </ul> <p>Even though the API is called \"Direct 3D 11\" weirdly enough we can't just simply render a bunch of vertices and have it show up as we expect it to look.  Because our screen is 2D, we need to be able to \"transform\" our 3D model into a 2D space.</p>"},{"location":"1-introduction/1-3-basics/1-3-3-setting-up-3d-rendering/#the-math","title":"The Math","text":"<p>For this we'll need to take a light dive into \"Matrix Math\", whilst understanding the math behind it all can be really helpful (especially once you start doing more advanced stuff), we'll use a library for all of this and only stick to top-level concepts as not to make this tutorial a math-lesson.</p> <p>The \"Transformation\" we're concerned with is composed out of a set of multiple matrices:</p> <p>For our 3D object we have a:</p> <ul> <li>Rotation matrix (contains the rotation)</li> <li>Scaling matrix (contains the scale)</li> <li>Translation matrix (contains the position)</li> </ul> <p>These three matrices will compose a matrix we call the \"Model matrix\", which we get by multiplying them together by doing:</p> <p><code>ModelMatrix = ((Rotation * Scaling) * Translation)</code>. </p> <p>This is one of the two key components to getting a 3D object on our screen.</p> <p>Next up we generally have a something we could call a \"Camera\" containing a:</p> <ul> <li>View matrix (contains the camera position + rotation information)</li> <li>Projection matrix</li> </ul> <p>The Projection matrix needs a bit more explanation, most traditional camera setups have two modi: \"Perspective\" and \"Orthographic\", this is what our \"projection matrix\" contains alongside, the \"Field of View\" or \"orthographic size\" and our \"Near\" and \"Far\" plane whose importance will be clear in a moment. </p> <p>Multiplying these two matrices together results in our \"ViewProjection matrix\" or \"Camera matrix\", this is the other key component for getting our 3D object on our screen, which we get by doing:</p> <p><code>ViewProjection = View * Projection</code>.</p> <p>Now that we have our Model matrix and ViewProjection we can make our final matrix, the \"world matrix\" which we get by multiplying them together: <code>WorldMatrix = ModelMatrix * ViewProjection</code> This matrix is what we transform every vertex with in order to get our 3D object on our 2D screen.</p> <p>The code to do all of this looks like this:</p> <p>Our camera will be defined by the view and projection matrix:     XMVECTOR camPos = XMLoadFloat3(&amp;_cameraPosition);</p> <pre><code>XMMATRIX view = XMMatrixLookAtLH(camPos, g_XMZero, { 0,1,0,1 });\nXMMATRIX proj = XMMatrixPerspectiveFovLH(90.0f * 0.0174533f, //degrees to radians\n                                        static_cast&lt;float&gt;(_width) / static_cast&lt;float&gt;(_height),\n                                        0.1f,\n                                        100.0f);\n//combine the view &amp; proj matrix\nXMMATRIX viewProjection = XMMatrixMultiply(view, proj);\n</code></pre> <p>And our 3D object will use the resulting model matrix:     XMMATRIX translation = XMMatrixTranslation(0, 0, 0);     XMMATRIX scaling = XMMatrixScaling(_scale, _scale, _scale);     XMMATRIX rotation = XMMatrixRotationRollPitchYaw(0, _yRotation, 0);</p> <pre><code>//Now we create our model matrix\nXMMATRIX modelMatrix = XMMatrixMultiply(translation, XMMatrixMultiply(scaling, rotation));\n</code></pre> <p>Because all these matrix multiplications happen infrequently enough, we \"can\" do this on the CPU, we only have to recalculate the matrices of 3D objects when they move/scale/rotate which for most level geometry is almost never. However...</p> <p>The only exception is the camera, which tends to move almost every frame, however we tend to only have 1 of them (or an insignificant amount in other cases). The keen readers might realize that because of the fact that we recalculate the camera matrix, we have to recalculate the world matrix for 'every' 3D object.</p> <p>What we cannot do however (or well, not with high-poly objects) is transform every vertex on the CPU with the world matrix, luckily GPU's are practically built for this and thus are very good at it.  But that means we need a way to get the matrix we need over there somehow.</p>"},{"location":"1-introduction/1-3-basics/1-3-3-setting-up-3d-rendering/#constant-buffers","title":"Constant Buffers","text":"<p>In D3D11 we have a thing called a \"Constant Buffer\", this is a special buffer that contains values that the GPU can expect not to change during a draw call, this means the values are \"constant\" or \"uniform\" for the entire shader invocation. This is a great place to put our matrix.</p> <p>In <code>CreateConstantBuffers()</code> we create our buffer pretty much the same as we did our vertex buffer back in Hello Triangle, except now in the BindFlags, we specify D3D11_BIND_FLAG::BIND_CONSTANT_BUFFER</p> <pre><code>D3D11_BUFFER_DESC desc{};\ndesc.Usage = D3D11_USAGE::D3D11_USAGE_DYNAMIC;\ndesc.BindFlags = D3D11_BIND_FLAG::D3D11_BIND_CONSTANT_BUFFER;\ndesc.ByteWidth = sizeof(PerFrameConstantBuffer);\ndesc.CPUAccessFlags = D3D11_CPU_ACCESS_WRITE;\n\n_device-&gt;CreateBuffer(&amp;desc, nullptr, &amp;_perFrameConstantBuffer);\n\ndesc.ByteWidth = sizeof(PerObjectConstantBuffer);\n_device-&gt;CreateBuffer(&amp;desc, nullptr, &amp;_perObjectConstantBuffer);\n</code></pre> <p>Note that we create two buffers, this is because it is best practice to update constant buffers as little as possible.</p> <p>So we have one that we adjust every frame, and one that we 'ideally' never have to update (or very little), an example of this could be static geometry as it'll still need a model matrix but we never have to update it after initial creation.</p> <p>In <code>Update()</code> we will update the contents of our constantbuffers :</p> <pre><code>D3D11_MAPPED_SUBRESOURCE mappedResource;\n_deviceContext-&gt;Map(_perFrameConstantBuffer.Get(), 0, D3D11_MAP::D3D11_MAP_WRITE_DISCARD, 0, &amp;mappedResource);\nmemcpy(mappedResource.pData, &amp;_perFrameConstantBufferData, sizeof(PerFrameConstantBuffer));\n_deviceContext-&gt;Unmap(_perFrameConstantBuffer.Get(), 0);\n\n_deviceContext-&gt;Map(_perObjectConstantBuffer.Get(), 0, D3D11_MAP::D3D11_MAP_WRITE_DISCARD, 0, &amp;mappedResource);\nmemcpy(mappedResource.pData, &amp;_perObjectConstantBufferData, sizeof(PerObjectConstantBuffer));\n_deviceContext-&gt;Unmap(_perObjectConstantBuffer.Get(), 0);\n</code></pre> <p>These functions will take the data in our \"...ConstantBufferData\" members and upload it to the GPU.</p> <p>The last thing we need to do is make our vertex shader aware of these buffers, easily done by calling ID3D11DeviceContext::VSSetConstantBuffers, note that we can set both slots at once by doing the following:</p> <pre><code>ID3D11Buffer* constantBuffers[2] =\n{\n    _perFrameConstantBuffer.Get(),\n    _perObjectConstantBuffer.Get()\n};\n\n_deviceContext-&gt;VSSetConstantBuffers(0, 2, constantBuffers);\n</code></pre> <p>That's all we need to do in order to get some data usable on the GPU, finally we now just need to have our vertex shader aware of this data and apply it.</p> <p>The syntax for this is a little bit different than we're used to in C/C++ but simple enough:</p> <pre><code>cbuffer PerFrame : register(b0)\n{\n    matrix viewprojection;\n};\n\ncbuffer PerObject : register(b1)\n{\n    matrix modelmatrix;\n};\n</code></pre> <p>We basically declare and define our structure in a single line, <code>cbuffer</code> tells the shader it will be a Constant Buffer (and expect the structure-like layout), followed by the name of the object <code>PerFrame</code>/<code>PerObject</code> and lastly which slot to expect it on <code>: register(b0)</code> after that we just tell it to expect a single row_major matrix in both buffers.</p> <p>Now we're free to use the data in the shader.</p> <pre><code>matrix world = mul(modelmatrix, viewprojection);\n</code></pre> <p>First we get our World Matrix by multiplying the model matrix with the viewproject as we noted before, then all that's left is to transform every vertex we get as an input and output that.</p> <pre><code>output.Position = mul(world, float4(input.Position, 1.0));\n</code></pre> <p>That's all we need to do in order to get our 3D object onto the screen!</p> <p></p> <p>However since it's still a single triangle, it won't be very obvious if it were static, so in <code>Update()</code> we're actually  rotating it around by increasing the rotation we have every frame:     _yRotation += _deltaTime;</p> <p>Next up, we'll get a nice cube going!</p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-4-3d-rendering/","title":"3D Rendering","text":"<p>Coming from the previous chapter, two changes have been made, instead of a single triangle, we've added a list of vertices and indices in order to create a cube mesh, and to make things more clear, we're relying on colors instead of a texture for now.</p> <p>Without any other changes to our main code, our cube will show up in 3D just as it did with our triangle, quite easy huh!</p> <p></p> <p>But something seems wrong, our triangle didn't look so weird, why are we looking inside the cube and seeing the other side sometimes? </p> <p>Well, we're still missing one critical part in order to get 3D rendering to work properly. </p>"},{"location":"1-introduction/1-3-basics/1-3-4-3d-rendering/#the-depth-buffer","title":"The Depth Buffer","text":"<p>In order to fix our weird looking cube we'll need a depth buffer, this is a special kind of render target that keeps track of the \"depth\" of each fragment on the screen, basically saying \"how far away is this pixel on the screen\". This depth buffer can then be used by special hardware on the GPU to see if the fragment we're working on is behind or in front of the previous fragment.</p> <p>This is called \"Depth Testing\", a very important concept within common rendering.</p> <p>So let's create our depth buffer which in D3D11 are called a <code>DepthStencil</code> and the handle for it being a <code>DepthStencilView</code> (a \"DSV\" as we'll call em from here on)</p> <p>In <code>CreateDepthStencilView()</code> we have the following code:</p> <pre><code>D3D11_TEXTURE2D_DESC texDesc{};\ntexDesc.Height = GetWindowHeight();\ntexDesc.Width = GetWindowWidth();\ntexDesc.ArraySize = 1;\ntexDesc.SampleDesc.Count = 1;\ntexDesc.MipLevels = 1;\ntexDesc.BindFlags = D3D11_BIND_DEPTH_STENCIL;\ntexDesc.Format = DXGI_FORMAT_R32_TYPELESS;\n\nID3D11Texture2D* texture = nullptr;\nif (FAILED(_device-&gt;CreateTexture2D(&amp;texDesc, nullptr, &amp;texture)))\n{\n    std::cout &lt;&lt; \"DXGI: Failed to create texture for DepthStencilView\\n\";\n    return;\n}\n\nD3D11_DEPTH_STENCIL_VIEW_DESC dsvDesc{};\ndsvDesc.Format = DXGI_FORMAT_D32_FLOAT;\ndsvDesc.ViewDimension = D3D11_DSV_DIMENSION_TEXTURE2D;\nif(FAILED(_device-&gt;CreateDepthStencilView(texture, &amp;dsvDesc, &amp;_depthTarget)))\n{\n    std::cout &lt;&lt; \"DXGI: Failed to create DepthStencilView\\n\";\n    texture-&gt;Release();\n    return;\n}\n\ntexture-&gt;Release();\n</code></pre> <p>Most of the setup for the DSV is very much the same as creating a texture, the only difference being the BindFlags and the Format. BindFlags needs to be <code>D3D11_BIND_DEPTH_STENCIL</code> in order to let D3D11 know that we're going to bind it as a DSV,  the only curious thing here is <code>DXGI_FORMAT_R32_TYPELESS</code> because the internal format for depth (even though we view it as a FLOAT) is not really a raw buffer full of floats, but a special format that can differ between GPUs, thankfully we don't need to be concerned with that and all we need to tell it is that it's \"typeless\".</p> <p>Lastly in the <code>dsvDesc</code> we actually have two commonly usable choices:</p> <ul> <li>DXGI_FORMAT_D32_FLOAT (with <code>DXGI_FORMAT_R32_TYPELESS</code> on the texture)</li> <li>DXGI_FORMAT_D24_UNORM_S8_UINT (with <code>DXGI_FORMAT_R24G8_TYPELESS</code> on the texture)</li> </ul> <p>The latter one will also reserve room for something called a \"Stencil\", which can be used for rendering techniques, we'll explain that one in a later chapter.</p> <p>One important thing we need to mention is that the DSV always needs to match in resolution with the bound RenderTargetView, so in our case we need to also make sure to resize it if we resize the window. However because there is no functionality to \"resize\" a DSV, the solution is to clean up our current one and create a new one with the matching resolution.</p> <p>In order to use this newly created DSV we need to set it using the last parameter in <code>ID3D11DeviceContext::OMSetRenderTargets()</code></p> <pre><code>_deviceContext-&gt;OMSetRenderTargets(1, _renderTarget.GetAddressOf(), _depthTarget.Get());\n</code></pre> <p>Just as with our rendertarget we also need to make sure to clear it every frame (using <code>ID3D11DeviceContext::ClearDepthStencilView()</code>)</p> <pre><code>_deviceContext-&gt;ClearDepthStencilView(_depthTarget.Get(), D3D11_CLEAR_FLAG::D3D11_CLEAR_DEPTH, 1.0f, 0);\n</code></pre> <p>Note that this takes a <code>D3D11_CLEAR_FLAG</code> and two more arguments, the <code>D3D11_CLEAR_FLAG</code> allows us to clear Depth and Stencil seperately or both at once by OR-ing them together. The final two arguments are the values to clear the DSV to, first the depth value and lastly the stencil value.</p>"},{"location":"1-introduction/1-3-basics/1-3-4-3d-rendering/#the-depth-state","title":"The Depth State","text":"<p>In order to actually use the DSV we just created we need to set up a <code>DepthStencilState</code>, the code for this is pretty simple, in <code>CreateDepthState()</code> we have the following:</p> <pre><code>D3D11_DEPTH_STENCIL_DESC depthDesc{};\ndepthDesc.DepthEnable = TRUE;\ndepthDesc.DepthFunc = D3D11_COMPARISON_FUNC::D3D11_COMPARISON_LESS;\ndepthDesc.DepthWriteMask = D3D11_DEPTH_WRITE_MASK_ALL;\n\n_device-&gt;CreateDepthStencilState(&amp;depthDesc, &amp;_depthState);\n</code></pre> <p><code>DepthEnable</code> is exactly what it sounds like, it \"enables\" the usage of the DSV. <code>DepthFunc</code> is what allows us to get Depth Testing, <code>D3D11_COMPARISON_LESS</code> tells it to only draw fragments that have a depth \"lesser\" than the current value (things that are in front of the current fragment) <code>DepthWriteMask</code> is what allows us to specify whether we should write to the DSV or not, we can only specify ALL or ZERO (which disables writing).</p> <p>Finally all we need to do is set it by calling <code>ID3D11DeviceContext::OMSetDepthStencilState()</code> </p> <pre><code>_deviceContext-&gt;OMSetDepthStencilState(_depthState.Get(), 0);\n</code></pre> <p>And.. That's it! Running the application now will cause the cube to show up all proper-like!</p> <p></p> <p>This is because it now uses depth to make sure whether the fragment we're drawing is actually in front of older ones, so we're no longer reliant on the order of the vertices we're drawing, which if we would, would make 3D rendering a 'lot' harder as we'd need to sort all polygons in depth every frame we move the camera around.</p> <p>From here on out you should be able to create your own renderer using all the knowledge you have received so far, as an extra the next chapters will show you how to load models and get some very nice debugging UI going! </p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-5-models/","title":"Models","text":"<p>Error</p> <p>Please note that this chapter is unfinished and will be finished up at some point in the future.</p> <p>After being able to load and display an 3D cube the next step is getting some more exciting stuff on screen.  Models are the next step!</p> <p>Just as with textures, there's no real support for what we call \"Models\" in D3D11 (or any other API), with the sole exception being D3D9 and their \"x\" format, which we can consider antique and very much deprecated.</p> <p>So we have to load models ourselves. But before we start we need to know what a model actually is. You might open a game and say \"this character is a model\" or \"this house is a model\", but that doesn't answer much as things can easily be a lot more complicated.</p> <p>Take this with a grain of salt because how a \"model\" is defined isn't concrete. But a model is generally composed out of a few things, each of them optional, some of them might exist in one format and not in others.</p> Meshes A collection of vertices and optionally indices Materials Some arbitrary collection of things that tell us what a mesh looks like Bones A collection of \"bones\" which can be a collection of vertices with influence amounts Animations Generally a collection of matrices for each set of bones ??? Other things, anything! Other models, Scripts, Camera's, Lights, Plumbusses, You name it! <p>Now you might think: \"If everything is optional, and models could have anything, how do we deal with this?\"</p> <p>The answer is simple: We don't!</p> <p>There is no software in existance that can load and/or display all model formats and any feature they can support. Generally only the features 'most' people care about are supported, which is generally meshes and materials, and often enough animations as well.</p> <p>So how are we going to load these vague and arbitrary \"models\"?</p> <p>Well just as with textures, we're going to use a library for it. A common choice for this is Assimp, it is not the fastest, but it is one of the more flexible ones.</p> <p>Error</p> <p>TODO: model loading</p> <p>Project on GitHub</p> <p>Next chapter</p>"},{"location":"1-introduction/1-3-basics/1-3-6-dear-imgui/","title":"Dear ImGui","text":"<p>Error</p> <p>This chapter is coming later, this chapter is going to show you how to use Dear ImGui which is an amazing library to have fast and flexible UI for in your program. Meanwhile please visit: https://github.com/ocornut/imgui to get a look at this great tool for development, a must-have library to control your renderer/game/editor and whatever else you can think of! For a look at the amazing things people do with it, see: https://github.com/ocornut/imgui/issues/5886.</p>"},{"location":"8-code/repository/","title":"Code Repository","text":""}]}